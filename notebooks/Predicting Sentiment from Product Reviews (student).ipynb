{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predicting sentiment from product reviews\n",
    "\n",
    "\n",
    "The goal of this first notebook is to explore logistic regression and feature engineering with existing Sklearn, Pandas, and Numpy functions.\n",
    "\n",
    "In this notebook you will use product review data from Amazon.com to predict whether the sentiments about a product (from its reviews) are positive or negative.\n",
    "\n",
    "* Do some feature engineering\n",
    "* Train a logistic regression model to predict the sentiment of product reviews.\n",
    "* Inspect the weights (coefficients) of a trained logistic regression model.\n",
    "* Make a prediction (both class and probability) of sentiment for a new product review.\n",
    "* Given the logistic regression weights, predictors and ground truth labels, write a function to compute the **accuracy** of the model.\n",
    "* Inspect the coefficients of the logistic regression model and interpret their meanings.\n",
    "* Compare multiple logistic regression models.\n",
    "* Train Stochastic Gradient Descent classifier with and without cross validation\n",
    "\n",
    "Let's get started!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data preparation\n",
    "\n",
    "We will use a dataset consisting of Amazon.com product reviews."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "products = pd.read_csv('../datasets/Amazon Product Reviews I.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>asins</th>\n",
       "      <th>brand</th>\n",
       "      <th>categories</th>\n",
       "      <th>colors</th>\n",
       "      <th>dateAdded</th>\n",
       "      <th>dateUpdated</th>\n",
       "      <th>dimension</th>\n",
       "      <th>ean</th>\n",
       "      <th>keys</th>\n",
       "      <th>...</th>\n",
       "      <th>reviews.rating</th>\n",
       "      <th>reviews.sourceURLs</th>\n",
       "      <th>reviews.text</th>\n",
       "      <th>reviews.title</th>\n",
       "      <th>reviews.userCity</th>\n",
       "      <th>reviews.userProvince</th>\n",
       "      <th>reviews.username</th>\n",
       "      <th>sizes</th>\n",
       "      <th>upc</th>\n",
       "      <th>weight</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AVpe7AsMilAPnD_xQ78G</td>\n",
       "      <td>B00QJDU3KY</td>\n",
       "      <td>Amazon</td>\n",
       "      <td>Amazon Devices,mazon.co.uk</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2016-03-08T20:21:53Z</td>\n",
       "      <td>2017-07-18T23:52:58Z</td>\n",
       "      <td>169 mm x 117 mm x 9.1 mm</td>\n",
       "      <td>NaN</td>\n",
       "      <td>kindlepaperwhite/b00qjdu3ky</td>\n",
       "      <td>...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>https://www.amazon.com/Kindle-Paperwhite-High-...</td>\n",
       "      <td>I initially had trouble deciding between the p...</td>\n",
       "      <td>Paperwhite voyage, no regrets!</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Cristina M</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>205 grams</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>AVpe7AsMilAPnD_xQ78G</td>\n",
       "      <td>B00QJDU3KY</td>\n",
       "      <td>Amazon</td>\n",
       "      <td>Amazon Devices,mazon.co.uk</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2016-03-08T20:21:53Z</td>\n",
       "      <td>2017-07-18T23:52:58Z</td>\n",
       "      <td>169 mm x 117 mm x 9.1 mm</td>\n",
       "      <td>NaN</td>\n",
       "      <td>kindlepaperwhite/b00qjdu3ky</td>\n",
       "      <td>...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>https://www.amazon.com/Kindle-Paperwhite-High-...</td>\n",
       "      <td>Allow me to preface this with a little history...</td>\n",
       "      <td>One Simply Could Not Ask For More</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Ricky</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>205 grams</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>AVpe7AsMilAPnD_xQ78G</td>\n",
       "      <td>B00QJDU3KY</td>\n",
       "      <td>Amazon</td>\n",
       "      <td>Amazon Devices,mazon.co.uk</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2016-03-08T20:21:53Z</td>\n",
       "      <td>2017-07-18T23:52:58Z</td>\n",
       "      <td>169 mm x 117 mm x 9.1 mm</td>\n",
       "      <td>NaN</td>\n",
       "      <td>kindlepaperwhite/b00qjdu3ky</td>\n",
       "      <td>...</td>\n",
       "      <td>4.0</td>\n",
       "      <td>https://www.amazon.com/Kindle-Paperwhite-High-...</td>\n",
       "      <td>I am enjoying it so far. Great for reading. Ha...</td>\n",
       "      <td>Great for those that just want an e-reader</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Tedd Gardiner</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>205 grams</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>AVpe7AsMilAPnD_xQ78G</td>\n",
       "      <td>B00QJDU3KY</td>\n",
       "      <td>Amazon</td>\n",
       "      <td>Amazon Devices,mazon.co.uk</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2016-03-08T20:21:53Z</td>\n",
       "      <td>2017-07-18T23:52:58Z</td>\n",
       "      <td>169 mm x 117 mm x 9.1 mm</td>\n",
       "      <td>NaN</td>\n",
       "      <td>kindlepaperwhite/b00qjdu3ky</td>\n",
       "      <td>...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>https://www.amazon.com/Kindle-Paperwhite-High-...</td>\n",
       "      <td>I bought one of the first Paperwhites and have...</td>\n",
       "      <td>Love / Hate relationship</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Dougal</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>205 grams</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>AVpe7AsMilAPnD_xQ78G</td>\n",
       "      <td>B00QJDU3KY</td>\n",
       "      <td>Amazon</td>\n",
       "      <td>Amazon Devices,mazon.co.uk</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2016-03-08T20:21:53Z</td>\n",
       "      <td>2017-07-18T23:52:58Z</td>\n",
       "      <td>169 mm x 117 mm x 9.1 mm</td>\n",
       "      <td>NaN</td>\n",
       "      <td>kindlepaperwhite/b00qjdu3ky</td>\n",
       "      <td>...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>https://www.amazon.com/Kindle-Paperwhite-High-...</td>\n",
       "      <td>I have to say upfront - I don't like coroporat...</td>\n",
       "      <td>I LOVE IT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Miljan David Tanic</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>205 grams</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 27 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                     id       asins   brand                  categories  \\\n",
       "0  AVpe7AsMilAPnD_xQ78G  B00QJDU3KY  Amazon  Amazon Devices,mazon.co.uk   \n",
       "1  AVpe7AsMilAPnD_xQ78G  B00QJDU3KY  Amazon  Amazon Devices,mazon.co.uk   \n",
       "2  AVpe7AsMilAPnD_xQ78G  B00QJDU3KY  Amazon  Amazon Devices,mazon.co.uk   \n",
       "3  AVpe7AsMilAPnD_xQ78G  B00QJDU3KY  Amazon  Amazon Devices,mazon.co.uk   \n",
       "4  AVpe7AsMilAPnD_xQ78G  B00QJDU3KY  Amazon  Amazon Devices,mazon.co.uk   \n",
       "\n",
       "  colors             dateAdded           dateUpdated  \\\n",
       "0    NaN  2016-03-08T20:21:53Z  2017-07-18T23:52:58Z   \n",
       "1    NaN  2016-03-08T20:21:53Z  2017-07-18T23:52:58Z   \n",
       "2    NaN  2016-03-08T20:21:53Z  2017-07-18T23:52:58Z   \n",
       "3    NaN  2016-03-08T20:21:53Z  2017-07-18T23:52:58Z   \n",
       "4    NaN  2016-03-08T20:21:53Z  2017-07-18T23:52:58Z   \n",
       "\n",
       "                  dimension  ean                         keys  ...  \\\n",
       "0  169 mm x 117 mm x 9.1 mm  NaN  kindlepaperwhite/b00qjdu3ky  ...   \n",
       "1  169 mm x 117 mm x 9.1 mm  NaN  kindlepaperwhite/b00qjdu3ky  ...   \n",
       "2  169 mm x 117 mm x 9.1 mm  NaN  kindlepaperwhite/b00qjdu3ky  ...   \n",
       "3  169 mm x 117 mm x 9.1 mm  NaN  kindlepaperwhite/b00qjdu3ky  ...   \n",
       "4  169 mm x 117 mm x 9.1 mm  NaN  kindlepaperwhite/b00qjdu3ky  ...   \n",
       "\n",
       "  reviews.rating                                 reviews.sourceURLs  \\\n",
       "0            5.0  https://www.amazon.com/Kindle-Paperwhite-High-...   \n",
       "1            5.0  https://www.amazon.com/Kindle-Paperwhite-High-...   \n",
       "2            4.0  https://www.amazon.com/Kindle-Paperwhite-High-...   \n",
       "3            5.0  https://www.amazon.com/Kindle-Paperwhite-High-...   \n",
       "4            5.0  https://www.amazon.com/Kindle-Paperwhite-High-...   \n",
       "\n",
       "                                        reviews.text  \\\n",
       "0  I initially had trouble deciding between the p...   \n",
       "1  Allow me to preface this with a little history...   \n",
       "2  I am enjoying it so far. Great for reading. Ha...   \n",
       "3  I bought one of the first Paperwhites and have...   \n",
       "4  I have to say upfront - I don't like coroporat...   \n",
       "\n",
       "                                reviews.title reviews.userCity  \\\n",
       "0              Paperwhite voyage, no regrets!              NaN   \n",
       "1           One Simply Could Not Ask For More              NaN   \n",
       "2  Great for those that just want an e-reader              NaN   \n",
       "3                    Love / Hate relationship              NaN   \n",
       "4                                   I LOVE IT              NaN   \n",
       "\n",
       "  reviews.userProvince    reviews.username  sizes upc     weight  \n",
       "0                  NaN          Cristina M    NaN NaN  205 grams  \n",
       "1                  NaN               Ricky    NaN NaN  205 grams  \n",
       "2                  NaN       Tedd Gardiner    NaN NaN  205 grams  \n",
       "3                  NaN              Dougal    NaN NaN  205 grams  \n",
       "4                  NaN  Miljan David Tanic    NaN NaN  205 grams  \n",
       "\n",
       "[5 rows x 27 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "products.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let us see a preview of what the dataset looks like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>reviews</th>\n",
       "      <th>rating</th>\n",
       "      <th>title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I initially had trouble deciding between the p...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>Paperwhite voyage, no regrets!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Allow me to preface this with a little history...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>One Simply Could Not Ask For More</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I am enjoying it so far. Great for reading. Ha...</td>\n",
       "      <td>4.0</td>\n",
       "      <td>Great for those that just want an e-reader</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>I bought one of the first Paperwhites and have...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>Love / Hate relationship</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>I have to say upfront - I don't like coroporat...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>I LOVE IT</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             reviews  rating  \\\n",
       "0  I initially had trouble deciding between the p...     5.0   \n",
       "1  Allow me to preface this with a little history...     5.0   \n",
       "2  I am enjoying it so far. Great for reading. Ha...     4.0   \n",
       "3  I bought one of the first Paperwhites and have...     5.0   \n",
       "4  I have to say upfront - I don't like coroporat...     5.0   \n",
       "\n",
       "                                        title  \n",
       "0              Paperwhite voyage, no regrets!  \n",
       "1           One Simply Could Not Ask For More  \n",
       "2  Great for those that just want an e-reader  \n",
       "3                    Love / Hate relationship  \n",
       "4                                   I LOVE IT  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Simplify relevant columns names\n",
    "if('reviews.rating' in products.columns):\n",
    "    products['rating']=products['reviews.rating']\n",
    "    products.drop(['reviews.rating'],axis=1, inplace=True)\n",
    "\n",
    "if('reviews.text' in products.columns):\n",
    "    products['reviews']=products['reviews.text']\n",
    "    products.drop(['reviews.text'],axis=1, inplace=True)\n",
    "    \n",
    "if('reviews.title' in products.columns):\n",
    "    products['title']=products['reviews.title']\n",
    "    products.drop(['reviews.title'],axis=1, inplace=True)\n",
    "\n",
    "# Drop irrelevant columns\n",
    "relevant_cols=['reviews','rating','title']\n",
    "products = products.loc[:, relevant_cols]\n",
    "\n",
    "# Drop Nana\n",
    "products.dropna(subset=['rating', 'reviews','title'], inplace=True)\n",
    "products.reset_index(drop=True, inplace=True)\n",
    "products.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build the word count vector for each review"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us explore a specific example of a Amazon product."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I can read a lot longer without pain in my hands since the cover holds it for me. Very nice shade of blue.\n"
     ]
    }
   ],
   "source": [
    "print(products['reviews'][269])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we will perform 2 simple data transformations:\n",
    "\n",
    "1. Remove punctuation using [Python's built-in](https://docs.python.org/2/library/string.html) string functionality.\n",
    "2. Transform the reviews into word-counts.\n",
    "\n",
    "**Aside**. In this notebook, we remove all punctuations for the sake of simplicity. A smarter approach to punctuations would preserve phrases such as \"I'd\", \"would've\", \"hadn't\" and so forth. See [this page](https://neptune.ai/blog/tokenization-in-nlp) for an example of smart handling of punctuations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0       I initially had trouble deciding between the p...\n",
      "1       Allow me to preface this with a little history...\n",
      "2       I am enjoying it so far Great for reading Had ...\n",
      "3       I bought one of the first Paperwhites and have...\n",
      "4       I have to say upfront  I dont like coroporate ...\n",
      "                              ...                        \n",
      "1172    This is not the same remote that I got for my ...\n",
      "1173    I have had to change the batteries in this rem...\n",
      "1174    Remote did not activate nor did it connect to ...\n",
      "1175    It does the job but is super over priced I fee...\n",
      "1176    I ordered this item to replace the one that no...\n",
      "Name: reviews, Length: 1177, dtype: object\n"
     ]
    }
   ],
   "source": [
    "import string \n",
    "def remove_punctuation(text):\n",
    "    try: # python 2.x\n",
    "        text = text.translate(None, string.punctuation) \n",
    "    except: # python 3.x\n",
    "        translator = text.maketrans('', '', string.punctuation)\n",
    "        text = text.translate(translator)\n",
    "    return text\n",
    "\n",
    "products['reviews'] = products['reviews'].apply(remove_punctuation)\n",
    "print(products['reviews'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1177, 6750)\n",
      "(1177, 6750)\n",
      "                                             reviews  rating  \\\n",
      "0  I initially had trouble deciding between the p...     5.0   \n",
      "1  Allow me to preface this with a little history...     5.0   \n",
      "2  I am enjoying it so far Great for reading Had ...     4.0   \n",
      "3  I bought one of the first Paperwhites and have...     5.0   \n",
      "4  I have to say upfront  I dont like coroporate ...     5.0   \n",
      "\n",
      "                                        title  word_count_0  word_count_1  \\\n",
      "0              Paperwhite voyage, no regrets!           0.0           0.0   \n",
      "1           One Simply Could Not Ask For More           0.0           0.0   \n",
      "2  Great for those that just want an e-reader           0.0           0.0   \n",
      "3                    Love / Hate relationship           0.0           0.0   \n",
      "4                                   I LOVE IT           0.0           0.0   \n",
      "\n",
      "   word_count_2  word_count_3  word_count_4  word_count_5  word_count_6  ...  \\\n",
      "0           0.0           0.0           0.0           0.0           0.0  ...   \n",
      "1           0.0           0.0           0.0           0.0           0.0  ...   \n",
      "2           0.0           0.0           0.0           0.0           0.0  ...   \n",
      "3           0.0           0.0           0.0           0.0           0.0  ...   \n",
      "4           0.0           0.0           0.0           0.0           0.0  ...   \n",
      "\n",
      "   word_count_6740  word_count_6741  word_count_6742  word_count_6743  \\\n",
      "0              0.0              0.0              0.0              0.0   \n",
      "1              0.0              0.0              0.0              0.0   \n",
      "2              0.0              0.0              0.0              0.0   \n",
      "3              0.0              0.0              0.0              0.0   \n",
      "4              0.0              0.0              0.0              0.0   \n",
      "\n",
      "   word_count_6744  word_count_6745  word_count_6746  word_count_6747  \\\n",
      "0              0.0              0.0              0.0              0.0   \n",
      "1              0.0              0.0              0.0              0.0   \n",
      "2              0.0              0.0              0.0              0.0   \n",
      "3              0.0              0.0              0.0              0.0   \n",
      "4              0.0              0.0              0.0              0.0   \n",
      "\n",
      "   word_count_6748  word_count_6749  \n",
      "0              0.0              0.0  \n",
      "1              0.0              0.0  \n",
      "2              0.0              0.0  \n",
      "3              0.0              0.0  \n",
      "4              0.0              0.0  \n",
      "\n",
      "[5 rows x 6753 columns]\n"
     ]
    }
   ],
   "source": [
    "# Frequency counts\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "count_vect = CountVectorizer()\n",
    "X_train_counts = count_vect.fit_transform(products['reviews'])\n",
    "print(X_train_counts.shape)\n",
    "\n",
    "# TF-IDF\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "tfidf_transformer = TfidfTransformer()\n",
    "X_train_tfidf = tfidf_transformer.fit_transform(X_train_counts)\n",
    "print(X_train_tfidf.shape)\n",
    "\n",
    "word_count_df = pd.DataFrame(X_train_tfidf.toarray())\n",
    "word_count_df = word_count_df.add_prefix('word_count_')\n",
    "products = pd.concat([products, word_count_df], axis=1)\n",
    "\n",
    "print(products.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let us explore what the sample example above looks like after these 2 transformations. Here, each entry in the **word_count** column is a dictionary where the key is the word and the value is a count of the number of times the word occurs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract sentiments\n",
    "\n",
    "We will **ignore** all reviews with *rating = 3*, since they tend to have a neutral sentiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1053, 6753)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "products = products[products['rating'] != 3]\n",
    "products.reset_index(drop=True, inplace=True)\n",
    "products.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we will assign reviews with a rating of 4 or higher to be *positive* reviews, while the ones with rating of 2 or lower are *negative*. For the sentiment column, we use +1 for the positive class label and -1 for the negative class label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>reviews</th>\n",
       "      <th>rating</th>\n",
       "      <th>title</th>\n",
       "      <th>word_count_0</th>\n",
       "      <th>word_count_1</th>\n",
       "      <th>word_count_2</th>\n",
       "      <th>word_count_3</th>\n",
       "      <th>word_count_4</th>\n",
       "      <th>word_count_5</th>\n",
       "      <th>word_count_6</th>\n",
       "      <th>...</th>\n",
       "      <th>word_count_6741</th>\n",
       "      <th>word_count_6742</th>\n",
       "      <th>word_count_6743</th>\n",
       "      <th>word_count_6744</th>\n",
       "      <th>word_count_6745</th>\n",
       "      <th>word_count_6746</th>\n",
       "      <th>word_count_6747</th>\n",
       "      <th>word_count_6748</th>\n",
       "      <th>word_count_6749</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I initially had trouble deciding between the p...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>Paperwhite voyage, no regrets!</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Allow me to preface this with a little history...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>One Simply Could Not Ask For More</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I am enjoying it so far Great for reading Had ...</td>\n",
       "      <td>4.0</td>\n",
       "      <td>Great for those that just want an e-reader</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>I bought one of the first Paperwhites and have...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>Love / Hate relationship</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>I have to say upfront  I dont like coroporate ...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>I LOVE IT</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 6754 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             reviews  rating  \\\n",
       "0  I initially had trouble deciding between the p...     5.0   \n",
       "1  Allow me to preface this with a little history...     5.0   \n",
       "2  I am enjoying it so far Great for reading Had ...     4.0   \n",
       "3  I bought one of the first Paperwhites and have...     5.0   \n",
       "4  I have to say upfront  I dont like coroporate ...     5.0   \n",
       "\n",
       "                                        title  word_count_0  word_count_1  \\\n",
       "0              Paperwhite voyage, no regrets!           0.0           0.0   \n",
       "1           One Simply Could Not Ask For More           0.0           0.0   \n",
       "2  Great for those that just want an e-reader           0.0           0.0   \n",
       "3                    Love / Hate relationship           0.0           0.0   \n",
       "4                                   I LOVE IT           0.0           0.0   \n",
       "\n",
       "   word_count_2  word_count_3  word_count_4  word_count_5  word_count_6  ...  \\\n",
       "0           0.0           0.0           0.0           0.0           0.0  ...   \n",
       "1           0.0           0.0           0.0           0.0           0.0  ...   \n",
       "2           0.0           0.0           0.0           0.0           0.0  ...   \n",
       "3           0.0           0.0           0.0           0.0           0.0  ...   \n",
       "4           0.0           0.0           0.0           0.0           0.0  ...   \n",
       "\n",
       "   word_count_6741  word_count_6742  word_count_6743  word_count_6744  \\\n",
       "0              0.0              0.0              0.0              0.0   \n",
       "1              0.0              0.0              0.0              0.0   \n",
       "2              0.0              0.0              0.0              0.0   \n",
       "3              0.0              0.0              0.0              0.0   \n",
       "4              0.0              0.0              0.0              0.0   \n",
       "\n",
       "   word_count_6745  word_count_6746  word_count_6747  word_count_6748  \\\n",
       "0              0.0              0.0              0.0              0.0   \n",
       "1              0.0              0.0              0.0              0.0   \n",
       "2              0.0              0.0              0.0              0.0   \n",
       "3              0.0              0.0              0.0              0.0   \n",
       "4              0.0              0.0              0.0              0.0   \n",
       "\n",
       "   word_count_6749  sentiment  \n",
       "0              0.0          1  \n",
       "1              0.0          1  \n",
       "2              0.0          1  \n",
       "3              0.0          1  \n",
       "4              0.0          1  \n",
       "\n",
       "[5 rows x 6754 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "products['sentiment'] = products['rating'].apply(lambda r : +1 if r > 3 else -1)\n",
    "products.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[<AxesSubplot:title={'center':'sentiment'}>]], dtype=object)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# View number of positive and negative sentiment\n",
    "products.hist(column=['sentiment'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can see that the dataset contains an extra column called **sentiment** which is either positive (+1) or negative (-1).\n",
    "\n",
    "Note, there are significantly more positive reviews than negative reviews."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Match number of positive and negative reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 977 positive reviews\n",
      "There are 76 negative reviews\n",
      "[Update] There are 76 positive reviews\n",
      "                                               reviews  rating  \\\n",
      "43   After 15 months my 500 tablet is no longer use...     1.0   \n",
      "69   This is a great controller for our Fire TV I p...     2.0   \n",
      "79   This is a great controller for our Fire TV I p...     2.0   \n",
      "84   The connection drops out on this unit pretty b...     1.0   \n",
      "117  As the Kids Edition is nothing more than a bas...     1.0   \n",
      "\n",
      "                                                 title  word_count_0  \\\n",
      "43                                Dead after 15 months           0.0   \n",
      "69   Great controller with 2 major problems! Not re...           0.0   \n",
      "79   Great controller with 2 major problems! Not re...           0.0   \n",
      "84   This needs to be way better for the amount I s...           0.0   \n",
      "117          Freetime makes me angry. So, so angry. 3,           0.0   \n",
      "\n",
      "     word_count_1  word_count_2  word_count_3  word_count_4  word_count_5  \\\n",
      "43            0.0           0.0           0.0           0.0           0.0   \n",
      "69            0.0           0.0           0.0           0.0           0.0   \n",
      "79            0.0           0.0           0.0           0.0           0.0   \n",
      "84            0.0           0.0           0.0           0.0           0.0   \n",
      "117           0.0           0.0           0.0           0.0           0.0   \n",
      "\n",
      "     word_count_6  ...  word_count_6741  word_count_6742  word_count_6743  \\\n",
      "43            0.0  ...              0.0              0.0              0.0   \n",
      "69            0.0  ...              0.0              0.0              0.0   \n",
      "79            0.0  ...              0.0              0.0              0.0   \n",
      "84            0.0  ...              0.0              0.0              0.0   \n",
      "117           0.0  ...              0.0              0.0              0.0   \n",
      "\n",
      "     word_count_6744  word_count_6745  word_count_6746  word_count_6747  \\\n",
      "43               0.0              0.0              0.0              0.0   \n",
      "69               0.0              0.0              0.0              0.0   \n",
      "79               0.0              0.0              0.0              0.0   \n",
      "84               0.0              0.0              0.0              0.0   \n",
      "117              0.0              0.0              0.0              0.0   \n",
      "\n",
      "     word_count_6748  word_count_6749  sentiment  \n",
      "43               0.0              0.0         -1  \n",
      "69               0.0              0.0         -1  \n",
      "79               0.0              0.0         -1  \n",
      "84               0.0              0.0         -1  \n",
      "117              0.0              0.0         -1  \n",
      "\n",
      "[5 rows x 6754 columns]\n"
     ]
    }
   ],
   "source": [
    "# Report number of positive examples\n",
    "positive_sent = products[products['sentiment']==1]\n",
    "print('There are {} positive reviews'.format(len(positive_sent)))\n",
    "\n",
    "# Report number of negative examples\n",
    "negative_sent = products[products['sentiment']==-1]\n",
    "print('There are {} negative reviews'.format(len(negative_sent)))\n",
    "\n",
    "# Sample number of negative example from positive examples using dr.sample()(# positive > # negative)\n",
    "positive_sample = positive_sent.sample(n = len(negative_sent))\n",
    "print('[Update] There are {} positive reviews'.format(len(positive_sample)))\n",
    "\n",
    "# Merge positive and negative examples and update products dataframe\n",
    "products = pd.concat([negative_sent, positive_sample])\n",
    "print(products.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split data into training and test sets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's perform a train/test split with 80% of the data in the training set and 20% of the data in the test set. We use `seed=1` so that everyone gets the same result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "121\n",
      "121\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = products.loc[:, ~products.columns.isin(['sentiment'])]\n",
    "\n",
    "y = products.loc[:, products.columns.isin(['sentiment'])]\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, \n",
    "                                                    y, \n",
    "                                                    test_size=.2, \n",
    "                                                    random_state=1)\n",
    "print(len(X_train))\n",
    "print(len(y_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train a sentiment classifier with logistic regression\n",
    "\n",
    "We will now use logistic regression to create a sentiment classifier on the training data. This model will use the column **word_count** as a feature and the column **sentiment** as the target. We will use `validation_set=None` to obtain same results as everyone else.\n",
    "\n",
    "**Note:** This line may take 1-2 minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word_count_0</th>\n",
       "      <th>word_count_1</th>\n",
       "      <th>word_count_2</th>\n",
       "      <th>word_count_3</th>\n",
       "      <th>word_count_4</th>\n",
       "      <th>word_count_5</th>\n",
       "      <th>word_count_6</th>\n",
       "      <th>word_count_7</th>\n",
       "      <th>word_count_8</th>\n",
       "      <th>word_count_9</th>\n",
       "      <th>...</th>\n",
       "      <th>word_count_6740</th>\n",
       "      <th>word_count_6741</th>\n",
       "      <th>word_count_6742</th>\n",
       "      <th>word_count_6743</th>\n",
       "      <th>word_count_6744</th>\n",
       "      <th>word_count_6745</th>\n",
       "      <th>word_count_6746</th>\n",
       "      <th>word_count_6747</th>\n",
       "      <th>word_count_6748</th>\n",
       "      <th>word_count_6749</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>229</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>551</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>556</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>490</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>547</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 6750 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     word_count_0  word_count_1  word_count_2  word_count_3  word_count_4  \\\n",
       "229           0.0           0.0           0.0           0.0           0.0   \n",
       "551           0.0           0.0           0.0           0.0           0.0   \n",
       "556           0.0           0.0           0.0           0.0           0.0   \n",
       "490           0.0           0.0           0.0           0.0           0.0   \n",
       "547           0.0           0.0           0.0           0.0           0.0   \n",
       "\n",
       "     word_count_5  word_count_6  word_count_7  word_count_8  word_count_9  \\\n",
       "229           0.0           0.0           0.0           0.0           0.0   \n",
       "551           0.0           0.0           0.0           0.0           0.0   \n",
       "556           0.0           0.0           0.0           0.0           0.0   \n",
       "490           0.0           0.0           0.0           0.0           0.0   \n",
       "547           0.0           0.0           0.0           0.0           0.0   \n",
       "\n",
       "     ...  word_count_6740  word_count_6741  word_count_6742  word_count_6743  \\\n",
       "229  ...              0.0              0.0              0.0              0.0   \n",
       "551  ...              0.0              0.0              0.0              0.0   \n",
       "556  ...              0.0              0.0              0.0              0.0   \n",
       "490  ...              0.0              0.0              0.0              0.0   \n",
       "547  ...              0.0              0.0              0.0              0.0   \n",
       "\n",
       "     word_count_6744  word_count_6745  word_count_6746  word_count_6747  \\\n",
       "229              0.0              0.0              0.0              0.0   \n",
       "551              0.0              0.0              0.0              0.0   \n",
       "556              0.0              0.0              0.0              0.0   \n",
       "490              0.0              0.0              0.0              0.0   \n",
       "547              0.0              0.0              0.0              0.0   \n",
       "\n",
       "     word_count_6748  word_count_6749  \n",
       "229              0.0              0.0  \n",
       "551              0.0              0.0  \n",
       "556              0.0              0.0  \n",
       "490              0.0              0.0  \n",
       "547              0.0              0.0  \n",
       "\n",
       "[5 rows x 6750 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_sentiment = X_train.loc[:,X_train.columns.str.startswith('word_count_')]\n",
    "X_test_sentiment = X_test.loc[:,X_test.columns.str.startswith('word_count_')]\n",
    "\n",
    "X_train_sentiment.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(random_state=0)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "sentiment_model = LogisticRegression(random_state=0)\n",
    "sentiment_model.fit(X_train_sentiment, np.ravel(y_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Aside**. You may get a warning to the effect of \"Terminated due to numerical difficulties --- this model may not be ideal\". It means that the quality metric (to be covered in Module 3) failed to improve in the last iteration of the run. The difficulty arises as the sentiment model puts too much weight on extremely rare words. A way to rectify this is to apply regularization, to be covered in Module 4. Regularization lessens the effect of extremely rare words. For the purpose of this assignment, however, please proceed with the model above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have fitted the model, we can extract the weights (coefficients) as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. ... 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "weights = sentiment_model.coef_\n",
    "print(weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are a total of `6750` coefficients in the model. Recall from the lecture that positive weights $w_j$ correspond to weights that cause positive sentiment, while negative weights correspond to negative sentiment. \n",
    "\n",
    "Fill in the following block of code to calculate how many *weights* are positive ( >= 0). (**Hint**: Use numpy to sum the weights and check *weights* must be positive ( >= 0))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of positive weights: 5692 \n",
      "Number of negative weights: 1058 \n"
     ]
    }
   ],
   "source": [
    "num_positive_weights = np.sum(weights >= 0)\n",
    "num_negative_weights = np.sum(weights < 0)\n",
    "\n",
    "print(\"Number of positive weights: %s \" % num_positive_weights)\n",
    "print(\"Number of negative weights: %s \" % num_negative_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Discussion Question:** How many weights are >= 0?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making predictions with logistic regression\n",
    "\n",
    "Now that a model is trained, we can make predictions on the **test data**. In this section, we will explore this in the context of 3 examples in the test dataset.  We refer to this set of 3 examples as the **sample_test_data**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1006    5.0\n",
      "916     4.0\n",
      "1015    5.0\n",
      "Name: rating, dtype: float64\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>reviews</th>\n",
       "      <th>rating</th>\n",
       "      <th>title</th>\n",
       "      <th>word_count_0</th>\n",
       "      <th>word_count_1</th>\n",
       "      <th>word_count_2</th>\n",
       "      <th>word_count_3</th>\n",
       "      <th>word_count_4</th>\n",
       "      <th>word_count_5</th>\n",
       "      <th>word_count_6</th>\n",
       "      <th>...</th>\n",
       "      <th>word_count_6740</th>\n",
       "      <th>word_count_6741</th>\n",
       "      <th>word_count_6742</th>\n",
       "      <th>word_count_6743</th>\n",
       "      <th>word_count_6744</th>\n",
       "      <th>word_count_6745</th>\n",
       "      <th>word_count_6746</th>\n",
       "      <th>word_count_6747</th>\n",
       "      <th>word_count_6748</th>\n",
       "      <th>word_count_6749</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1006</th>\n",
       "      <td>I bought these for a couple of reasonsFirst I ...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>I hate having to shove headphones into my brai...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>916</th>\n",
       "      <td>Ive enjoyed using the features Alexa app and l...</td>\n",
       "      <td>4.0</td>\n",
       "      <td>Great item</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1015</th>\n",
       "      <td>I bought these for a couple of reasonsFirst I ...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>I hate having to shove headphones into my brai...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 6753 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                reviews  rating  \\\n",
       "1006  I bought these for a couple of reasonsFirst I ...     5.0   \n",
       "916   Ive enjoyed using the features Alexa app and l...     4.0   \n",
       "1015  I bought these for a couple of reasonsFirst I ...     5.0   \n",
       "\n",
       "                                                  title  word_count_0  \\\n",
       "1006  I hate having to shove headphones into my brai...           0.0   \n",
       "916                                          Great item           0.0   \n",
       "1015  I hate having to shove headphones into my brai...           0.0   \n",
       "\n",
       "      word_count_1  word_count_2  word_count_3  word_count_4  word_count_5  \\\n",
       "1006           0.0           0.0           0.0           0.0           0.0   \n",
       "916            0.0           0.0           0.0           0.0           0.0   \n",
       "1015           0.0           0.0           0.0           0.0           0.0   \n",
       "\n",
       "      word_count_6  ...  word_count_6740  word_count_6741  word_count_6742  \\\n",
       "1006           0.0  ...              0.0              0.0              0.0   \n",
       "916            0.0  ...              0.0              0.0              0.0   \n",
       "1015           0.0  ...              0.0              0.0              0.0   \n",
       "\n",
       "      word_count_6743  word_count_6744  word_count_6745  word_count_6746  \\\n",
       "1006              0.0              0.0              0.0              0.0   \n",
       "916               0.0              0.0              0.0              0.0   \n",
       "1015              0.0              0.0              0.0              0.0   \n",
       "\n",
       "      word_count_6747  word_count_6748  word_count_6749  \n",
       "1006              0.0              0.0              0.0  \n",
       "916               0.0              0.0              0.0  \n",
       "1015              0.0              0.0              0.0  \n",
       "\n",
       "[3 rows x 6753 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_test_data = X_test[10:13]\n",
    "print(sample_test_data['rating'])\n",
    "sample_test_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's dig deeper into the first row of the **sample_test_data**. Here's the full review:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I bought these for a couple of reasonsFirst I wanted an earbud that was NOT designed as noise cancelling I hate having to shove headphones into my brain to use them Apparently a lot of people are under the assumption that these are supposed to be noise cancelling or something which they are not They are simple earbuds I hate not being able to hear what is going on around me and dont feel safe in public using headphones that block everything else out These are designed just like the old style earbuds that you stick in your ears but they are more comfortable Maybe I have small ears or something but they could never fall out or dislodge from my ears They dont cancel the outside noise but they sit in my ears exactly as they are supposed to so the sound travels right into my earsSecond I really like the magnets and tangle free aspects of the headphones I use headphones while I work and the magnets allow me to hang the headphones easily when I am finished with them The flat cord and magnets also mean the headphones never really tangle when you put them in a bag or wherever Awesome designFinally the sound I get out of these seems pretty average I would compare them to the stock Apple iPhone earbuds or a set of Sony earbuds Nothing special They are only 25 and a lot of that is the fact these were just released and that you are paying for the design as far as magnets in the ear buds and the nice flat cable Dont expect them to perform like 100 headphones because they arent designed to perform like that They are just a nice simple set of earbuds that will sound good to most people The magnets and tangle free cord are awesome and they are really quite comfortableRead more'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_test_data['reviews'].iloc[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That review seems pretty positive.\n",
    "\n",
    "Now, let's see what the next row of the **sample_test_data** looks like. As we could guess from the sentiment (-1), the review is quite negative."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Ive enjoyed using the features Alexa app and like the portability that this unit offers The sound quality is good'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_test_data['reviews'].iloc[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now make a **class** prediction for the **sample_test_data**. The `sentiment_model` should predict **+1** if the sentiment is positive and **-1** if the sentiment is negative. Recall from the lecture that the **score** (sometimes called **margin**) for the logistic regression model  is defined as:\n",
    "\n",
    "$$\n",
    "\\mbox{score}_i = \\mathbf{w}^T h(\\mathbf{x}_i)\n",
    "$$ \n",
    "\n",
    "where $h(\\mathbf{x}_i)$ represents the features for example $i$.  We will write some code to obtain the **scores**. For each row, the **score** (or margin) is a number in the range **[-inf, inf]**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.61675798 0.52691388 0.61675798]\n"
     ]
    }
   ],
   "source": [
    "scores = sample_test_data.loc[:, sample_test_data.columns.str.startswith('word_count_')].to_numpy() @ weights.reshape((-1)) + sentiment_model.intercept_\n",
    "print(scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predicting sentiment\n",
    "\n",
    "These scores can be used to make class predictions as follows:\n",
    "\n",
    "$$\n",
    "\\hat{y} = \n",
    "\\left\\{\n",
    "\\begin{array}{ll}\n",
    "      +1 & \\mathbf{w}^T h(\\mathbf{x}_i) > 0 \\\\\n",
    "      -1 & \\mathbf{w}^T h(\\mathbf{x}_i) \\leq 0 \\\\\n",
    "\\end{array} \n",
    "\\right.\n",
    "$$\n",
    "\n",
    "Using scores, write code to calculate $\\hat{y}$, the class predictions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 1 1]\n"
     ]
    }
   ],
   "source": [
    "y_pred = np.where(scores > 0, 1, -1)\n",
    "print(y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the following code to verify that the class predictions obtained by your calculations are the same as that obtained above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class predictions:\n",
      "[1 1 1]\n"
     ]
    }
   ],
   "source": [
    "print(\"Class predictions:\")\n",
    "print(sentiment_model.predict(\n",
    "    sample_test_data.loc[:, sample_test_data.columns.str.startswith('word_count_')]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Checkpoint**: Make sure your class predictions match with the one obtained from above.\n",
    "\n",
    "### Probability predictions\n",
    "\n",
    "Recall from the lectures that we can also calculate the probability predictions from the scores using:\n",
    "$$\n",
    "P(y_i = +1 | \\mathbf{x}_i,\\mathbf{w}) = \\frac{1}{1 + \\exp(-\\mathbf{w}^T h(\\mathbf{x}_i))}.\n",
    "$$\n",
    "\n",
    "Using the variable **scores** calculated previously, write code to calculate the probability that a sentiment is positive using the above formula. For each row, the probabilities should be a number in the range **[0, 1]**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Checkpoint**: Make sure your probability predictions match the ones obtained from sklearn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Discussion Question:** Of the three data points in **sample_test_data**, which one (first, second, or third) has the **lowest probability** of being classified as a positive review?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Find the most positive (and negative) review"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now turn to examining the full test dataset, **test_data**, and use Numpy argsort to form predictions on all of the test data points for faster performance.\n",
    "\n",
    "Using the `sentiment_model`, find the 20 reviews in the entire **test_data** with the **highest probability** of being classified as a **positive review**. We refer to these as the \"most positive reviews.\"\n",
    "\n",
    "To calculate these top-20 reviews, use the following steps:\n",
    "1.  Make probability predictions on **test_data** using the `sentiment_model`. (**Hint:** When you call `.predict` to make predictions on the test data.)\n",
    "2.  Sort the data according to those predictions and pick the top 20. (**Hint:** You can use indexing [-topn:] to find the top k rows sorted according to the value of a specified column.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[29 19  1 27 13  4  8 26 23 25 14 16 24 18  7  9  2 30 28  0]\n",
      "                                                reviews  rating  \\\n",
      "120   This is my 4th Kindle Fire I also have the fir...     5.0   \n",
      "740   Didnt want to spend 200 on the Echo for someon...     5.0   \n",
      "512   Love the Dot Hate the Tap Portable This device...     2.0   \n",
      "842   I was not overly impressed with this system I ...     2.0   \n",
      "15    Im a firsttime Kindle owner so I have nothing ...     4.0   \n",
      "469   Just really thought it would have been more us...     2.0   \n",
      "968   As good as the Echo is the Echo Voice Remote s...     2.0   \n",
      "267   In this day and age of rectangles with screens...     2.0   \n",
      "643   Amazon is the tip of leading technology Always...     5.0   \n",
      "776   hard to use and does not support Guam online i...     1.0   \n",
      "134   DONT BUY a reburbished Kindle Ive had to retur...     1.0   \n",
      "217                                                 Too     2.0   \n",
      "960   I tried 2 other screen protectors that claimed...     5.0   \n",
      "65    Major fan didnt buy this because of protection...     5.0   \n",
      "213                                     very overpriced     2.0   \n",
      "876   Could not get it to work properly I put the ap...     1.0   \n",
      "1046  Dont bother paying for one or getting a replac...     2.0   \n",
      "591   Im thinking this must of been a fluke it never...     2.0   \n",
      "971   Worked great for a few weeks then completely s...     1.0   \n",
      "1033  I dont know whats wrong with fire tv remotes b...     1.0   \n",
      "\n",
      "                                                  title  word_count_0  \\\n",
      "120   Keeps Getting Better, Especially for Comic Rea...           0.0   \n",
      "740                    Great for those new to Bluetooth           0.0   \n",
      "512                 Love the Dot, Hate the Tap Portable           0.0   \n",
      "842                             I returned this product           0.0   \n",
      "15    I Wanted a Dedicated E-Reader, and That's What...           0.0   \n",
      "469                                       Not impressed           0.0   \n",
      "968   Has Trouble Understanding Commands, Volume Con...           0.0   \n",
      "267                                            Just Bad           0.0   \n",
      "643                      I really enjoy Amazon Products           0.0   \n",
      "776                                  Not a good product           0.0   \n",
      "134                     Don't buy a refurbished Kindle!           0.0   \n",
      "217                                           Two Stars           0.0   \n",
      "960   Don't waste time with other screen protectors,...           0.0   \n",
      "65    Major fan, didn't buy this because of protecti...           0.0   \n",
      "213                                           Two Stars           0.0   \n",
      "876                            Would not work properly!           0.0   \n",
      "1046                                      Don't Buy It.           0.0   \n",
      "591                                     It never worked           0.0   \n",
      "971                                  Dead after 3 weeks           0.0   \n",
      "1033                       Amazon fire tv remote sucks!           0.0   \n",
      "\n",
      "      word_count_1  word_count_2  word_count_3  word_count_4  word_count_5  \\\n",
      "120            0.0           0.0           0.0           0.0           0.0   \n",
      "740            0.0           0.0           0.0           0.0           0.0   \n",
      "512            0.0           0.0           0.0           0.0           0.0   \n",
      "842            0.0           0.0           0.0           0.0           0.0   \n",
      "15             0.0           0.0           0.0           0.0           0.0   \n",
      "469            0.0           0.0           0.0           0.0           0.0   \n",
      "968            0.0           0.0           0.0           0.0           0.0   \n",
      "267            0.0           0.0           0.0           0.0           0.0   \n",
      "643            0.0           0.0           0.0           0.0           0.0   \n",
      "776            0.0           0.0           0.0           0.0           0.0   \n",
      "134            0.0           0.0           0.0           0.0           0.0   \n",
      "217            0.0           0.0           0.0           0.0           0.0   \n",
      "960            0.0           0.0           0.0           0.0           0.0   \n",
      "65             0.0           0.0           0.0           0.0           0.0   \n",
      "213            0.0           0.0           0.0           0.0           0.0   \n",
      "876            0.0           0.0           0.0           0.0           0.0   \n",
      "1046           0.0           0.0           0.0           0.0           0.0   \n",
      "591            0.0           0.0           0.0           0.0           0.0   \n",
      "971            0.0           0.0           0.0           0.0           0.0   \n",
      "1033           0.0           0.0           0.0           0.0           0.0   \n",
      "\n",
      "      word_count_6  ...  word_count_6740  word_count_6741  word_count_6742  \\\n",
      "120            0.0  ...              0.0              0.0              0.0   \n",
      "740            0.0  ...              0.0              0.0              0.0   \n",
      "512            0.0  ...              0.0              0.0              0.0   \n",
      "842            0.0  ...              0.0              0.0              0.0   \n",
      "15             0.0  ...              0.0              0.0              0.0   \n",
      "469            0.0  ...              0.0              0.0              0.0   \n",
      "968            0.0  ...              0.0              0.0              0.0   \n",
      "267            0.0  ...              0.0              0.0              0.0   \n",
      "643            0.0  ...              0.0              0.0              0.0   \n",
      "776            0.0  ...              0.0              0.0              0.0   \n",
      "134            0.0  ...              0.0              0.0              0.0   \n",
      "217            0.0  ...              0.0              0.0              0.0   \n",
      "960            0.0  ...              0.0              0.0              0.0   \n",
      "65             0.0  ...              0.0              0.0              0.0   \n",
      "213            0.0  ...              0.0              0.0              0.0   \n",
      "876            0.0  ...              0.0              0.0              0.0   \n",
      "1046           0.0  ...              0.0              0.0              0.0   \n",
      "591            0.0  ...              0.0              0.0              0.0   \n",
      "971            0.0  ...              0.0              0.0              0.0   \n",
      "1033           0.0  ...              0.0              0.0              0.0   \n",
      "\n",
      "      word_count_6743  word_count_6744  word_count_6745  word_count_6746  \\\n",
      "120               0.0              0.0              0.0              0.0   \n",
      "740               0.0              0.0              0.0              0.0   \n",
      "512               0.0              0.0              0.0              0.0   \n",
      "842               0.0              0.0              0.0              0.0   \n",
      "15                0.0              0.0              0.0              0.0   \n",
      "469               0.0              0.0              0.0              0.0   \n",
      "968               0.0              0.0              0.0              0.0   \n",
      "267               0.0              0.0              0.0              0.0   \n",
      "643               0.0              0.0              0.0              0.0   \n",
      "776               0.0              0.0              0.0              0.0   \n",
      "134               0.0              0.0              0.0              0.0   \n",
      "217               0.0              0.0              0.0              0.0   \n",
      "960               0.0              0.0              0.0              0.0   \n",
      "65                0.0              0.0              0.0              0.0   \n",
      "213               0.0              0.0              0.0              0.0   \n",
      "876               0.0              0.0              0.0              0.0   \n",
      "1046              0.0              0.0              0.0              0.0   \n",
      "591               0.0              0.0              0.0              0.0   \n",
      "971               0.0              0.0              0.0              0.0   \n",
      "1033              0.0              0.0              0.0              0.0   \n",
      "\n",
      "      word_count_6747  word_count_6748  word_count_6749  \n",
      "120               0.0              0.0              0.0  \n",
      "740               0.0              0.0              0.0  \n",
      "512               0.0              0.0              0.0  \n",
      "842               0.0              0.0              0.0  \n",
      "15                0.0              0.0              0.0  \n",
      "469               0.0              0.0              0.0  \n",
      "968               0.0              0.0              0.0  \n",
      "267               0.0              0.0              0.0  \n",
      "643               0.0              0.0              0.0  \n",
      "776               0.0              0.0              0.0  \n",
      "134               0.0              0.0              0.0  \n",
      "217               0.0              0.0              0.0  \n",
      "960               0.0              0.0              0.0  \n",
      "65                0.0              0.0              0.0  \n",
      "213               0.0              0.0              0.0  \n",
      "876               0.0              0.0              0.0  \n",
      "1046              0.0              0.0              0.0  \n",
      "591               0.0              0.0              0.0  \n",
      "971               0.0              0.0              0.0  \n",
      "1033              0.0              0.0              0.0  \n",
      "\n",
      "[20 rows x 6753 columns]\n"
     ]
    }
   ],
   "source": [
    "# Calculate the class probabilities for the test set\n",
    "y_prob = sentiment_model.predict_proba(X_test_sentiment)\n",
    "\n",
    "# Sort the test set in descending order of their probabilities of being positive\n",
    "idx = np.argsort(-y_prob[:, 1])\n",
    "\n",
    "# Get the indices of the 20 most positive reviews\n",
    "idx_most_positive = idx[-20:] \n",
    "print(idx_most_positive)\n",
    "\n",
    "# Get the corresponding reviews from the test set\n",
    "most_positive_reviews = X_test.iloc[idx_most_positive]\n",
    "print(most_positive_reviews)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Discussion Question**: Which of the following products are represented in the 20 most positive reviews? [multiple choice]\n",
    "\n",
    "\n",
    "Now, let us repeat this exercise to find the \"most negative reviews.\" Use the prediction probabilities to find the  20 reviews in the **test_data** with the **lowest probability** of being classified as a **positive review**. Repeat the same steps above but make sure you **sort in the opposite order**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                reviews  rating  \\\n",
      "1006  I bought these for a couple of reasonsFirst I ...     5.0   \n",
      "1015  I bought these for a couple of reasonsFirst I ...     5.0   \n",
      "916   Ive enjoyed using the features Alexa app and l...     4.0   \n",
      "539   I have an Alexa but wanted a speaker for trave...     4.0   \n",
      "722   I purchased the Amazon Tap when it was on sale...     5.0   \n",
      "390   Writing for the person it was bought for she l...     5.0   \n",
      "475   Great concept but speaker not so good at full ...     4.0   \n",
      "151   Okay if you dont want to read my whole review ...     5.0   \n",
      "930   My 8 yr old daughter loves her Tap She uses it...     5.0   \n",
      "672   Great product with a ton of features Add the E...     5.0   \n",
      "886   Great little device easy to connect to bluetoo...     5.0   \n",
      "120   This is my 4th Kindle Fire I also have the fir...     5.0   \n",
      "740   Didnt want to spend 200 on the Echo for someon...     5.0   \n",
      "512   Love the Dot Hate the Tap Portable This device...     2.0   \n",
      "842   I was not overly impressed with this system I ...     2.0   \n",
      "15    Im a firsttime Kindle owner so I have nothing ...     4.0   \n",
      "469   Just really thought it would have been more us...     2.0   \n",
      "968   As good as the Echo is the Echo Voice Remote s...     2.0   \n",
      "267   In this day and age of rectangles with screens...     2.0   \n",
      "643   Amazon is the tip of leading technology Always...     5.0   \n",
      "\n",
      "                                                  title  word_count_0  \\\n",
      "1006  I hate having to shove headphones into my brai...           0.0   \n",
      "1015  I hate having to shove headphones into my brai...           0.0   \n",
      "916                                          Great item           0.0   \n",
      "539                           So much fun for traveling           0.0   \n",
      "722               Amazing Sound and great Battery Life!           0.0   \n",
      "390                  Parkinson's Disabled and LOVES it!           0.0   \n",
      "475                                       Great concept           0.0   \n",
      "151   A superb e-ink reader, but is it worth the mon...           0.0   \n",
      "930                           Awesome portable speaker.           0.0   \n",
      "672                Great product with a ton of features           0.0   \n",
      "886                             Easy small decent sound           0.0   \n",
      "120   Keeps Getting Better, Especially for Comic Rea...           0.0   \n",
      "740                    Great for those new to Bluetooth           0.0   \n",
      "512                 Love the Dot, Hate the Tap Portable           0.0   \n",
      "842                             I returned this product           0.0   \n",
      "15    I Wanted a Dedicated E-Reader, and That's What...           0.0   \n",
      "469                                       Not impressed           0.0   \n",
      "968   Has Trouble Understanding Commands, Volume Con...           0.0   \n",
      "267                                            Just Bad           0.0   \n",
      "643                      I really enjoy Amazon Products           0.0   \n",
      "\n",
      "      word_count_1  word_count_2  word_count_3  word_count_4  word_count_5  \\\n",
      "1006           0.0           0.0           0.0           0.0           0.0   \n",
      "1015           0.0           0.0           0.0           0.0           0.0   \n",
      "916            0.0           0.0           0.0           0.0           0.0   \n",
      "539            0.0           0.0           0.0           0.0           0.0   \n",
      "722            0.0           0.0           0.0           0.0           0.0   \n",
      "390            0.0           0.0           0.0           0.0           0.0   \n",
      "475            0.0           0.0           0.0           0.0           0.0   \n",
      "151            0.0           0.0           0.0           0.0           0.0   \n",
      "930            0.0           0.0           0.0           0.0           0.0   \n",
      "672            0.0           0.0           0.0           0.0           0.0   \n",
      "886            0.0           0.0           0.0           0.0           0.0   \n",
      "120            0.0           0.0           0.0           0.0           0.0   \n",
      "740            0.0           0.0           0.0           0.0           0.0   \n",
      "512            0.0           0.0           0.0           0.0           0.0   \n",
      "842            0.0           0.0           0.0           0.0           0.0   \n",
      "15             0.0           0.0           0.0           0.0           0.0   \n",
      "469            0.0           0.0           0.0           0.0           0.0   \n",
      "968            0.0           0.0           0.0           0.0           0.0   \n",
      "267            0.0           0.0           0.0           0.0           0.0   \n",
      "643            0.0           0.0           0.0           0.0           0.0   \n",
      "\n",
      "      word_count_6  ...  word_count_6740  word_count_6741  word_count_6742  \\\n",
      "1006           0.0  ...              0.0              0.0              0.0   \n",
      "1015           0.0  ...              0.0              0.0              0.0   \n",
      "916            0.0  ...              0.0              0.0              0.0   \n",
      "539            0.0  ...              0.0              0.0              0.0   \n",
      "722            0.0  ...              0.0              0.0              0.0   \n",
      "390            0.0  ...              0.0              0.0              0.0   \n",
      "475            0.0  ...              0.0              0.0              0.0   \n",
      "151            0.0  ...              0.0              0.0              0.0   \n",
      "930            0.0  ...              0.0              0.0              0.0   \n",
      "672            0.0  ...              0.0              0.0              0.0   \n",
      "886            0.0  ...              0.0              0.0              0.0   \n",
      "120            0.0  ...              0.0              0.0              0.0   \n",
      "740            0.0  ...              0.0              0.0              0.0   \n",
      "512            0.0  ...              0.0              0.0              0.0   \n",
      "842            0.0  ...              0.0              0.0              0.0   \n",
      "15             0.0  ...              0.0              0.0              0.0   \n",
      "469            0.0  ...              0.0              0.0              0.0   \n",
      "968            0.0  ...              0.0              0.0              0.0   \n",
      "267            0.0  ...              0.0              0.0              0.0   \n",
      "643            0.0  ...              0.0              0.0              0.0   \n",
      "\n",
      "      word_count_6743  word_count_6744  word_count_6745  word_count_6746  \\\n",
      "1006              0.0         0.000000              0.0              0.0   \n",
      "1015              0.0         0.000000              0.0              0.0   \n",
      "916               0.0         0.000000              0.0              0.0   \n",
      "539               0.0         0.000000              0.0              0.0   \n",
      "722               0.0         0.000000              0.0              0.0   \n",
      "390               0.0         0.000000              0.0              0.0   \n",
      "475               0.0         0.000000              0.0              0.0   \n",
      "151               0.0         0.000000              0.0              0.0   \n",
      "930               0.0         0.300829              0.0              0.0   \n",
      "672               0.0         0.000000              0.0              0.0   \n",
      "886               0.0         0.000000              0.0              0.0   \n",
      "120               0.0         0.000000              0.0              0.0   \n",
      "740               0.0         0.000000              0.0              0.0   \n",
      "512               0.0         0.000000              0.0              0.0   \n",
      "842               0.0         0.000000              0.0              0.0   \n",
      "15                0.0         0.000000              0.0              0.0   \n",
      "469               0.0         0.000000              0.0              0.0   \n",
      "968               0.0         0.000000              0.0              0.0   \n",
      "267               0.0         0.000000              0.0              0.0   \n",
      "643               0.0         0.000000              0.0              0.0   \n",
      "\n",
      "      word_count_6747  word_count_6748  word_count_6749  \n",
      "1006              0.0              0.0              0.0  \n",
      "1015              0.0              0.0              0.0  \n",
      "916               0.0              0.0              0.0  \n",
      "539               0.0              0.0              0.0  \n",
      "722               0.0              0.0              0.0  \n",
      "390               0.0              0.0              0.0  \n",
      "475               0.0              0.0              0.0  \n",
      "151               0.0              0.0              0.0  \n",
      "930               0.0              0.0              0.0  \n",
      "672               0.0              0.0              0.0  \n",
      "886               0.0              0.0              0.0  \n",
      "120               0.0              0.0              0.0  \n",
      "740               0.0              0.0              0.0  \n",
      "512               0.0              0.0              0.0  \n",
      "842               0.0              0.0              0.0  \n",
      "15                0.0              0.0              0.0  \n",
      "469               0.0              0.0              0.0  \n",
      "968               0.0              0.0              0.0  \n",
      "267               0.0              0.0              0.0  \n",
      "643               0.0              0.0              0.0  \n",
      "\n",
      "[20 rows x 6753 columns]\n"
     ]
    }
   ],
   "source": [
    "# Get the indices of the 20 most negative reviews\n",
    "idx_most_negative = idx[:20]\n",
    "\n",
    "# Get the corresponding reviews from the test set\n",
    "most_negative_reviews = X_test.iloc[idx_most_negative]\n",
    "print(most_negative_reviews)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "**Question**: Which of the following products are represented in the 20 most negative reviews?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute accuracy of the classifier\n",
    "\n",
    "We will now evaluate the accuracy of the trained classifier. Recall that the accuracy is given by\n",
    "\n",
    "\n",
    "$$\n",
    "\\mbox{accuracy} = \\frac{\\mbox{# correctly classified examples}}{\\mbox{# total examples}}\n",
    "$$\n",
    "\n",
    "This can be computed as follows:\n",
    "\n",
    "* **Step 1:** Use the trained model to compute class predictions (**Hint:** Use the `predict` method)\n",
    "* **Step 2:** Count the number of data points when the predicted class labels match the ground truth labels (called `true_labels` below).\n",
    "* **Step 3:** Divide the total number of correct predictions by the total number of data points in the dataset.\n",
    "\n",
    "Complete the function below to compute the classification accuracy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def get_classification_accuracy(model, data, true_labels):\n",
    "    # First get the predictions\n",
    "    ## add code here\n",
    "    \n",
    "    # Compute the number of correctly classified examples\n",
    "    ## add code here\n",
    "\n",
    "    # Then compute accuracy by dividing num_correct by total number of examples\n",
    "    ## add code here\n",
    "    \n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's compute the classification accuracy of the **sentiment_model** on the **test_data**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#add code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Discussion Question**: What is the accuracy of the **sentiment_model** on the **test_data**? Round your answer to 2 decimal places (e.g. 0.76).\n",
    "\n",
    "**Discussion Question**: Does a higher accuracy value on the **training_data** always imply that the classifier is better?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learn another classifier with fewer words\n",
    "\n",
    "There were a lot of words in the model we trained above. We will now train a simpler logistic regression model using only a subset of words that occur in the reviews. For this assignment, we selected a 20 words to work with. These are:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "significant_words = ['love', 'great', 'easy', 'old', 'amazing', 'perfect', 'loves', \n",
    "      'well', 'able', 'car', 'broke', 'less', 'even', 'waste', 'disappointed', \n",
    "      'work', 'product', 'money', 'would', 'return']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(significant_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#all_words = count_vect.get_feature_names_out() # newer version of sklearn\n",
    "all_words = count_vect.get_feature_names()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each review, we will use the **word_count** column and trim out all words that are **not** in the **significant_words** list above. We will use the [SArray dictionary trim by keys functionality]( https://dato.com/products/create/docs/generated/graphlab.SArray.dict_trim_by_keys.html). Note that we are performing this on both the training and test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word_count_310</th>\n",
       "      <th>word_count_515</th>\n",
       "      <th>word_count_1016</th>\n",
       "      <th>word_count_1137</th>\n",
       "      <th>word_count_1838</th>\n",
       "      <th>word_count_2039</th>\n",
       "      <th>word_count_2192</th>\n",
       "      <th>word_count_2764</th>\n",
       "      <th>word_count_3522</th>\n",
       "      <th>word_count_3662</th>\n",
       "      <th>word_count_3666</th>\n",
       "      <th>word_count_3901</th>\n",
       "      <th>word_count_4164</th>\n",
       "      <th>word_count_4416</th>\n",
       "      <th>word_count_4680</th>\n",
       "      <th>word_count_5069</th>\n",
       "      <th>word_count_6489</th>\n",
       "      <th>word_count_6538</th>\n",
       "      <th>word_count_6648</th>\n",
       "      <th>word_count_6671</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>229</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.108244</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.155978</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>551</th>\n",
       "      <td>0.122733</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>556</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.271011</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.347796</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.368185</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>490</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.063354</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>547</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     word_count_310  word_count_515  word_count_1016  word_count_1137  \\\n",
       "229        0.000000             0.0              0.0              0.0   \n",
       "551        0.122733             0.0              0.0              0.0   \n",
       "556        0.000000             0.0              0.0              0.0   \n",
       "490        0.000000             0.0              0.0              0.0   \n",
       "547        0.000000             0.0              0.0              0.0   \n",
       "\n",
       "     word_count_1838  word_count_2039  word_count_2192  word_count_2764  \\\n",
       "229              0.0              0.0              0.0         0.000000   \n",
       "551              0.0              0.0              0.0         0.000000   \n",
       "556              0.0              0.0              0.0         0.271011   \n",
       "490              0.0              0.0              0.0         0.063354   \n",
       "547              0.0              0.0              0.0         0.000000   \n",
       "\n",
       "     word_count_3522  word_count_3662  word_count_3666  word_count_3901  \\\n",
       "229              0.0         0.000000              0.0         0.108244   \n",
       "551              0.0         0.000000              0.0         0.000000   \n",
       "556              0.0         0.347796              0.0         0.000000   \n",
       "490              0.0         0.000000              0.0         0.000000   \n",
       "547              0.0         0.000000              0.0         0.000000   \n",
       "\n",
       "     word_count_4164  word_count_4416  word_count_4680  word_count_5069  \\\n",
       "229              0.0              0.0         0.000000              0.0   \n",
       "551              0.0              0.0         0.000000              0.0   \n",
       "556              0.0              0.0         0.368185              0.0   \n",
       "490              0.0              0.0         0.000000              0.0   \n",
       "547              0.0              0.0         0.000000              0.0   \n",
       "\n",
       "     word_count_6489  word_count_6538  word_count_6648  word_count_6671  \n",
       "229         0.155978              0.0              0.0              0.0  \n",
       "551         0.000000              0.0              0.0              0.0  \n",
       "556         0.000000              0.0              0.0              0.0  \n",
       "490         0.000000              0.0              0.0              0.0  \n",
       "547         0.000000              0.0              0.0              0.0  "
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Extract significant words and split train/test sets\n",
    "\n",
    "sig_word_set = set(significant_words)\n",
    "sig_idx = [i for i, e in enumerate(all_words) if e in sig_word_set]\n",
    "\n",
    "X_train_sig = X_train_sentiment.iloc[:, sig_idx]\n",
    "X_test_sig = X_test_sentiment.iloc[:, sig_idx]\n",
    "\n",
    "X_train_sig.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see what the first example of the dataset looks like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I typically dont post reviews however dont waste your money on this case I got it on Friday and started using it that evening Its now Thursday and the case is peeling at the edges the fabric around the camera hole has started bubbling and the felt on the inside edge is wearing off i have not had any issues with it standing on its own like other people have mentioned I am giving this a two star mainly because it is still functional I am tempted to send it back but I like the functionality of this case'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train['reviews'].iloc[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **word_count** column had been working with before looks like the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "word_count_0       0.0\n",
       "word_count_1       0.0\n",
       "word_count_2       0.0\n",
       "word_count_3       0.0\n",
       "word_count_4       0.0\n",
       "                  ... \n",
       "word_count_6745    0.0\n",
       "word_count_6746    0.0\n",
       "word_count_6747    0.0\n",
       "word_count_6748    0.0\n",
       "word_count_6749    0.0\n",
       "Name: 229, Length: 6750, dtype: float64"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_sentiment.iloc[0, :]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we are only working with a subset of these words, the column **word_count_subset** is a subset of the above dictionary. In this example, only 2 `significant words` are present in this review."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "word_count_310     0.000000\n",
       "word_count_515     0.000000\n",
       "word_count_1016    0.000000\n",
       "word_count_1137    0.000000\n",
       "word_count_1838    0.000000\n",
       "word_count_2039    0.000000\n",
       "word_count_2192    0.000000\n",
       "word_count_2764    0.000000\n",
       "word_count_3522    0.000000\n",
       "word_count_3662    0.000000\n",
       "word_count_3666    0.000000\n",
       "word_count_3901    0.108244\n",
       "word_count_4164    0.000000\n",
       "word_count_4416    0.000000\n",
       "word_count_4680    0.000000\n",
       "word_count_5069    0.000000\n",
       "word_count_6489    0.155978\n",
       "word_count_6538    0.000000\n",
       "word_count_6648    0.000000\n",
       "word_count_6671    0.000000\n",
       "Name: 229, dtype: float64"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_sig.iloc[0, :]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train a logistic regression model on a subset of data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now build a classifier with **word_count_subset** with significant words as the feature and **sentiment** as the target. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can compute the classification accuracy using the `get_classification_accuracy` function you implemented earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we will inspect the weights (coefficients) of the **simple_model**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'simple_model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-38-43ed7c571f2c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msimple_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcoef_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'simple_model' is not defined"
     ]
    }
   ],
   "source": [
    "print(simple_model.coef_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's sort the coefficients (in descending order) by the **value** to obtain the coefficients with the most positive effect on the sentiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(-np.sort(-simple_model.coef_, ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Discussion Question**: Consider the coefficients of **simple_model**. There should be 21 of them, an intercept term + one for each word in **significant_words**. How many of the 20 coefficients (corresponding to the 20 **significant_words** and *excluding the intercept term*) are positive for the `simple_model`?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_positive_weights = np.sum(simple_model.coef_ >= 0)\n",
    "print(num_positive_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Discussion Question**: Are the positive words in the **simple_model** (let us call them `positive_significant_words`) also positive words in the **sentiment_model**?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparing models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now compare the accuracy of the **sentiment_model** and the **simple_model** using the `get_classification_accuracy` method you implemented above.\n",
    "\n",
    "First, compute the classification accuracy of the **sentiment_model** on the **train_data**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, compute the classification accuracy of the **simple_model** on the **train_data**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Discussion Question**: Which model (**sentiment_model** or **simple_model**) has higher accuracy on the TRAINING set?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we will repeat this exercise on the **test_data**. Start by computing the classification accuracy of the **sentiment_model** on the **test_data**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will compute the classification accuracy of the **simple_model** on the **test_data**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Discussion Question**: Which model (**sentiment_model** or **simple_model**) has higher accuracy on the TEST set?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Discussion Question**: Comment out the sention on 'Match number of positive and negative reviews' and re-run the notebook. Which model (**sentiment_model** or **simple_model**) has higher accuracy on the TEST set? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline: Majority class prediction\n",
    "\n",
    "It is quite common to use the **majority class classifier** as the a baseline (or reference) model for comparison with your classifier model. The majority classifier model predicts the majority class for all data points. At the very least, you should healthily beat the majority class classifier, otherwise, the model is (usually) pointless.\n",
    "\n",
    "What is the majority class in the **train_data**?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now compute the accuracy of the majority class classifier on **test_data**.\n",
    "\n",
    "**Discussion Question**: Enter the accuracy of the majority class classifier model on the **test_data**. Round your answer to two decimal places (e.g. 0.76)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# add code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Discussion Question**: Is the **sentiment_model** definitely better than the majority class classifier (the baseline)?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Logistic Regression via Stochastic Gradient Descent\n",
    "\n",
    "The goal of this notebook is to implement a logistic regression classifier using stochastic gradient descent. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Hyperparams\n",
    "max_iter=1000           # number of passes on training data\n",
    "tol=1e-3                # stopping criteria for iterations\n",
    "penalty='l2'            # 'l1' and 'l2' regularization term\n",
    "alpha=0.001             # Constant that multiplies the regularization term. Ranges from [0 Inf)\n",
    "loss='log'              # 'log' is logistic regression, 'hinge' for Support Vector Machine\n",
    "random_state=42         # seed of the pseudo random number generated which is used while shuffling the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "# Create SGD model\n",
    "sgd_model = SGDClassifier(max_iter=max_iter, \n",
    "                  tol=tol,\n",
    "                  penalty=penalty,\n",
    "                  alpha=alpha,\n",
    "                  loss=loss,\n",
    "                  random_state=random_state)\n",
    "sgd_model.fit(X_train_sentiment, np.ravel(y_train))\n",
    "\n",
    "sentiment_predictions = sgd_model.predict(X_test_sentiment.loc[:, X_test_sentiment.columns.str.startswith('word_count_')])\n",
    "sentiment_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_predictions = sgd_model.predict(X_test_sentiment.loc[:, X_test_sentiment.columns.str.startswith('word_count_')])\n",
    "sentiment_predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inspect SGD coefficients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stochastic Gradient Descent with Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stochastic Gradient Descent with Cross Validation\n",
    "from sklearn.model_selection import RepeatedKFold\n",
    "from sklearn.model_selection import cross_val_score, cross_validate\n",
    "\n",
    "# evaluation method\n",
    "n_splits=4\n",
    "n_repeats=5\n",
    "sgd_cv = RepeatedKFold(n_splits=n_splits, n_repeats=n_repeats, random_state=1)\n",
    "\n",
    "# define model\n",
    "sgd_model = SGDClassifier(max_iter=max_iter, \n",
    "                              tol=tol,\n",
    "                              penalty=penalty,\n",
    "                              alpha=alpha,\n",
    "                              loss=loss,\n",
    "                              random_state=random_state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate cross validation scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sgd_results = cross_validate(sgd_model, X_train_sentiment, np.ravel(y_train), scoring='accuracy',cv=sgd_cv, n_jobs=-1, return_estimator=True)\n",
    "\n",
    "best_model_idx = np.argmin(sgd_results['test_score'])\n",
    "estimator = sgd_results['estimator'][best_model_idx]\n",
    "print(estimator.coef_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# force scores to be positive\n",
    "scores = abs(sgd_results['test_score'])\n",
    "print('Accuracy: %.3f (%.3f)' % (np.mean(scores), np.std(scores)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Discussion Question** Explore effects of L2 regularization\n",
    "\n",
    "Now that we have written up all the pieces needed for regularized logistic regression, let's explore the benefits of using **L2 regularization** in analyzing sentiment for product reviews. **As iterations pass, the log likelihood should increase**.\n",
    "\n",
    "Below, we train models with increasing amounts of regularization, starting with no L2 penalty, which is equivalent to our previous logistic regression implementation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Confusion Matrix\n",
    "\n",
    "The accuracy, while convenient, does not tell the whole story. For a fuller picture, we turn to the **confusion matrix**. In the case of binary classification, the confusion matrix is a 2-by-2 matrix laying out correct and incorrect predictions made in each label as follows:\n",
    "```\n",
    "              +---------------------------------------------+\n",
    "              |                Predicted label              |\n",
    "              +----------------------+----------------------+\n",
    "              |          (+1)        |         (-1)         |\n",
    "+-------+-----+----------------------+----------------------+\n",
    "| True  |(+1) | # of true positive  | # of false negative |\n",
    "| label +-----+----------------------+----------------------+\n",
    "|       |(-1) | # of false positive | # of true negative  |\n",
    "+-------+-----+----------------------+----------------------+\n",
    "```\n",
    "To print out the confusion matrix for a classifier, use `metric='confusion_matrix'`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "sentiment_predictions = sentiment_model.predict(X_test_sentiment.loc[:, X_test_sentiment.columns.str.startswith('word_count_')])\n",
    "cmatrix = confusion_matrix(y_test, sentiment_predictions)\n",
    "cmatrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Discussion Question**: How many predicted values in the **test set** are **false positives**?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Computing the cost of mistakes\n",
    "\n",
    "Put yourself in the shoes of a manufacturer that sells a product on Amazon.com and you want to monitor your product's reviews in order to respond to complaints.  Even a few negative reviews may generate a lot of bad publicity about the product. So you don't want to miss any reviews with negative sentiments --- you'd rather put up with false alarms about potentially negative reviews instead of missing negative reviews entirely. In other words, **false positives cost more than false negatives**. (It may be the other way around for other scenarios, but let's stick with the manufacturer's scenario for now.)\n",
    "\n",
    "Suppose you know the costs involved in each kind of mistake: \n",
    "1. \\$100 for each false positive.\n",
    "2. \\$1 for each false negative.\n",
    "3. Correctly classified reviews incur no cost.\n",
    "\n",
    "**Discussion Question**: Given the stipulation, what is the cost associated with the logistic regression classifier's performance on the **test set**?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Add code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Precision and Recall\n",
    "\n",
    "You may not have exact dollar amounts for each kind of mistake. Instead, you may simply prefer to reduce the percentage of false positives to be less than, say, 3.5% of all positive predictions. This is where **precision** comes in:\n",
    "\n",
    "$$\n",
    "[\\text{precision}] = \\frac{[\\text{# positive data points with positive predicitions}]}{\\text{[# all data points with positive predictions]}} = \\frac{[\\text{# true positives}]}{[\\text{# true positives}] + [\\text{# false positives}]}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So to keep the percentage of false positives below 3.5% of positive predictions, we must raise the precision to 96.5% or higher. \n",
    "\n",
    "**First**, let us compute the precision of the logistic regression classifier on the **test_data**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Discussion Question**: Out of all reviews in the **test set** that are predicted to be positive, what fraction of them are **false positives**? (Round to the second decimal place e.g. 0.25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A complementary metric is **recall**, which measures the ratio between the number of true positives and that of (ground-truth) positive reviews:\n",
    "\n",
    "$$\n",
    "[\\text{recall}] = \\frac{[\\text{# positive data points with positive predicitions}]}{\\text{[# all positive data points]}} = \\frac{[\\text{# true positives}]}{[\\text{# true positives}] + [\\text{# false negatives}]}\n",
    "$$\n",
    "\n",
    "Let us compute the recall on the **test_data**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Discussion Question**: What fraction of the positive reviews in the **test_set** were correctly predicted as positive by the classifier?\n",
    "\n",
    "**Discussion Question**: What is the recall value for a classifier that predicts **+1** for all data points in the **test_data**?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Precision-recall tradeoff\n",
    "\n",
    "In this part, we will explore the trade-off between precision and recall discussed in the lecture.  We first examine what happens when we use a different threshold value for making class predictions.  We then explore a range of threshold values and plot the associated precision-recall curve.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Varying the threshold\n",
    "\n",
    "False positives are costly in our example, so we may want to be more conservative about making positive predictions. To achieve this, instead of thresholding class probabilities at 0.5, we can choose a higher threshold. \n",
    "\n",
    "Write a function called `apply_threshold` that accepts two things\n",
    "* `probabilities` (an SArray of probability values)\n",
    "* `threshold` (a float between 0 and 1).\n",
    "\n",
    "The function should return an SArray, where each element is set to +1 or -1 depending whether the corresponding probability exceeds `threshold`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_threshold(probabilities, threshold):\n",
    "    ### Add code here\n",
    "    # +1 if >= threshold and -1 otherwise."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run prediction with `output_type='probability'` to get the list of probability values. Then use thresholds set at 0.5 (default) and 0.9 to make predictions from these probability values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "probabilities = sentiment_model.predict_proba(X_test_sentiment)\n",
    "predictions_with_default_threshold = apply_threshold(probabilities, 0.5)\n",
    "predictions_with_high_threshold = apply_threshold(probabilities, 0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Number of positive predicted reviews (threshold = 0.5): %s\" % (predictions_with_default_threshold == 1).sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Number of positive predicted reviews (threshold = 0.9): %s\" % (predictions_with_high_threshold == 1).sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Discussion Question**: What happens to the number of positive predicted reviews as the threshold increased from 0.5 to 0.9?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploring the associated precision and recall as the threshold varies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By changing the probability threshold, it is possible to influence precision and recall. We can explore this as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import recall_score, precision_score\n",
    "# Threshold = 0.5\n",
    "precision_with_default_threshold = precision_score(\n",
    "    y_test, predictions_with_default_threshold)\n",
    "\n",
    "recall_with_default_threshold = recall_score(\n",
    "    y_test, predictions_with_default_threshold\n",
    ")\n",
    "\n",
    "\n",
    "# Threshold = 0.9\n",
    "precision_with_high_threshold = precision_score(\n",
    "    y_test, predictions_with_high_threshold, zero_division=1)\n",
    "\n",
    "recall_with_high_threshold = recall_score(\n",
    "    y_test, predictions_with_high_threshold\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Precision (threshold = 0.5): %s\" % precision_with_default_threshold)\n",
    "print(\"Recall (threshold = 0.5)   : %s\" % recall_with_default_threshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Precision (threshold = 0.9): %s\" % precision_with_high_threshold)\n",
    "print(\"Recall (threshold = 0.9)   : %s\" % recall_with_high_threshold)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Discussion Question (variant 1)**: Does the **precision** increase with a higher threshold?\n",
    "\n",
    "**Discussion Question (variant 2)**: Does the **recall** increase with a higher threshold?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Precision-recall curve\n",
    "\n",
    "Now, we will explore various different values of tresholds, compute the precision and recall scores, and then plot the precision-recall curve."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold_values = np.linspace(0.5, 1, num=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each of the values of threshold, we compute the precision and recall scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "precision_all = []\n",
    "recall_all = []\n",
    "\n",
    "probabilities = sentiment_model.predict_proba(X_test_sentiment)\n",
    "for threshold in threshold_values:\n",
    "    predictions = apply_threshold(probabilities, threshold)\n",
    "\n",
    "    precision = precision_score(y_test, predictions, zero_division=1)\n",
    "\n",
    "    recall = recall_score(y_test, predictions)\n",
    "    \n",
    "    precision_all.append(precision)\n",
    "    recall_all.append(recall)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's plot the precision-recall curve to visualize the precision-recall tradeoff as we vary the threshold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "def plot_pr_curve(precision, recall, title):\n",
    "    plt.rcParams['figure.figsize'] = 7, 5\n",
    "    plt.locator_params(axis = 'x', nbins = 5)\n",
    "    plt.plot(recall, precision, 'b-', linewidth=4.0, color = '#B0017F')\n",
    "    plt.title(title)\n",
    "    plt.ylabel('Precision')\n",
    "    plt.xlabel('Recall')\n",
    "    plt.rcParams.update({'font.size': 16})\n",
    "    \n",
    "plot_pr_curve(precision_all, recall_all, 'Precision recall curve (all)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Discussion Question**: Among all the threshold values tried, what is the **smallest** threshold value that achieves a precision of 96.5% or better? Round your answer to 3 decimal places."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Discussion Question**: Using `threshold` = 0.98, how many **false negatives** do we get on the **test_data**? (**Hint**: You may use the Sklearn precision and recall functions.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the number of false negatives (i.e the number of reviews to look at when not needed) that we have to deal with using this classifier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluating specific search terms\n",
    "\n",
    "So far, we looked at the number of false positives for the **entire test set**. In this section, let's select reviews using a specific search term and optimize the precision on these reviews only. After all, a manufacturer would be interested in tuning the false positive rate just for their products (the reviews they want to read) rather than that of the entire set of products on Amazon.\n",
    "\n",
    "## Precision-Recall on all tv related items\n",
    "\n",
    "From the **test set**, select all the reviews for all products with the word 'tv' in them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's predict the probability of classifying these reviews as positive:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plot the precision-recall curve for the **tv_reviews** dataset.\n",
    "\n",
    "**First**, let's consider the following `threshold_values` ranging from 0.5 to 1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Second**, as we did above, let's compute precision and recall for each value in `threshold_values` on the **tv_reviews** dataset.  Complete the code block below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "precision_all = []\n",
    "recall_all = []\n",
    "\n",
    "for threshold in threshold_values:    \n",
    "    # Make predictions. Use the `apply_threshold` function \n",
    "    ## Add code here\n",
    "\n",
    "    # Calculate the precision.\n",
    "    # Add code here\n",
    "    \n",
    "    # Add code here\n",
    "    \n",
    "    # Append the precision and recall scores."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Discussion Question**: Among all the threshold values tried, what is the **smallest** threshold value that achieves a precision of 96.5% or better for the reviews of data in **products**? Round your answer to 3 decimal places."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Discussion Question:** Is this threshold value smaller or larger than the threshold used for the entire dataset to achieve the same specified precision of 96.5%?\n",
    "\n",
    "**Finally**, let's plot the precision recall curve."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "vscode": {
   "interpreter": {
    "hash": "85463d1bb2888979eba71184c8d462df281b59dd3aecce6d9ffe8745b11b3329"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
