{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predicting sentiment from product reviews\n",
    "\n",
    "\n",
    "The goal of this first notebook is to explore logistic regression and feature engineering with existing Sklearn, Pandas, and Numpy functions.\n",
    "\n",
    "In this notebook you will use product review data from Amazon.com to predict whether the sentiments about a product (from its reviews) are positive or negative.\n",
    "\n",
    "* Do some feature engineering\n",
    "* Train a logistic regression model to predict the sentiment of product reviews.\n",
    "* Inspect the weights (coefficients) of a trained logistic regression model.\n",
    "* Make a prediction (both class and probability) of sentiment for a new product review.\n",
    "* Given the logistic regression weights, predictors and ground truth labels, write a function to compute the **accuracy** of the model.\n",
    "* Inspect the coefficients of the logistic regression model and interpret their meanings.\n",
    "* Compare multiple logistic regression models.\n",
    "\n",
    "Let's get started!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data preparation\n",
    "\n",
    "We will use a dataset consisting of Amazon.com product reviews."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Alexa Voice Remote for Amazon Echo and Echo Dot',\n",
       "       'Alexa Voice Remote for Amazon Fire TV and Fire TV Stick',\n",
       "       'All-New Amazon Fire 7 Tablet Case (7th Generation',\n",
       "       'All-New Amazon Fire HD 8 Tablet Case (7th Generation',\n",
       "       'All-New Amazon Fire TV Game Controller',\n",
       "       'All-New Amazon Kid-Proof Case for Amazon Fire 7 Tablet (7th Generation',\n",
       "       'All-New Amazon Kid-Proof Case for Amazon Fire HD 8 Tablet (7th Generation',\n",
       "       'All-New Fire 7 Kids Edition Tablet',\n",
       "       'All-New Fire 7 Tablet with Alexa',\n",
       "       'All-New Fire HD 8 Kids Edition Tablet',\n",
       "       'All-New Fire HD 8 Tablet with Alexa',\n",
       "       'Amazon 5W USB Official OEM Charger and Power Adapter for Fire Tablets and Kindle eReaders',\n",
       "       'Amazon Echo - Black',\n",
       "       'Amazon Echo Dot Case (fits Echo Dot 2nd Generation only) - Charcoal Fabric',\n",
       "       'Amazon Echo Dot Case (fits Echo Dot 2nd Generation only) - Indigo Fabric',\n",
       "       'Amazon Echo Dot Case (fits Echo Dot 2nd Generation only) - Merlot Leather',\n",
       "       'Amazon Echo Dot Case (fits Echo Dot 2nd Generation only) - Midnight Leather',\n",
       "       'Amazon Echo Dot Case (fits Echo Dot 2nd Generation only) - Saddle Tan Leather',\n",
       "       'Amazon Echo Dot Case (fits Echo Dot 2nd Generation only) - Sandstone Fabric',\n",
       "       'Amazon Fire TV', 'Amazon Fire TV Game Controller',\n",
       "       'Amazon Kindle Oasis Premium Leather Battery Cover - Black',\n",
       "       'Amazon Kindle Oasis Premium Leather Battery Cover - Walnut',\n",
       "       'Amazon Premium Headphones',\n",
       "       'Amazon Tap - Alexa-Enabled Portable Bluetooth Speaker',\n",
       "       'Amazon Tap Sling Cover - Blue', 'Amazon Tap Sling Cover - Green',\n",
       "       'Amazon Tap Sling Cover - Magenta',\n",
       "       'Amazon Tap Sling Cover - Tangerine',\n",
       "       'Amazon Tap Sling Cover - White',\n",
       "       'Certified Refurbished Amazon Fire TV (Previous Generation - 1st)',\n",
       "       'Certified Refurbished Echo Dot (2nd Generation) - Black',\n",
       "       'Certified Refurbished Fire HD 10 Tablet',\n",
       "       'Certified Refurbished Kindle E-reader',\n",
       "       'Certified Refurbished Kindle E-reader - Black',\n",
       "       'Certified Refurbished Kindle Paperwhite E-reader - Black',\n",
       "       'Certified Refurbished Kindle Voyage E-reader with Special Offers',\n",
       "       'Echo Dot (2nd Generation) - Black', 'Echo Show - Black',\n",
       "       'Fire HD 10 Tablet with Alexa', 'Fire HD 6 Tablet',\n",
       "       'Fire HD 7 Tablet', 'Fire HD 8 Tablet',\n",
       "       'Fire HD 8 Tablet with Alexa', 'Fire HDX 8.9 Tablet',\n",
       "       'Fire Kids Edition Tablet', 'Fire Tablet with Alexa', 'Kindle',\n",
       "       'Kindle E-reader - Black', 'Kindle Fire HD 7\"',\n",
       "       'Kindle Fire HDX 7\"', 'Kindle Fire HDX 8.9\"', 'Kindle Keyboard',\n",
       "       'Kindle Oasis E-reader with Leather Charging Cover - Walnut',\n",
       "       'Kindle Oasis with Leather Charging Cover - Black',\n",
       "       'Kindle Paperwhite', 'Kindle Paperwhite 3G',\n",
       "       'Kindle Paperwhite E-reader - Black', 'Kindle Voyage E-reader',\n",
       "       'Kindle for Kids Bundle with the latest Kindle E-reader',\n",
       "       'Moshi Anti-Glare No Bubble Screen Protector for the Fire Phone',\n",
       "       'Replacement Remote for Amazon Fire TV Stick'], dtype=object)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "products = pd.read_csv('../datasets/Amazon Product Reviews I.csv')\n",
    "np.unique(products['name'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let us see a preview of what the dataset looks like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                             reviews  rating  \\\n",
      "0  I initially had trouble deciding between the p...     5.0   \n",
      "1  Allow me to preface this with a little history...     5.0   \n",
      "2  I am enjoying it so far. Great for reading. Ha...     4.0   \n",
      "3  I bought one of the first Paperwhites and have...     5.0   \n",
      "4  I have to say upfront - I don't like coroporat...     5.0   \n",
      "\n",
      "                                        title  \n",
      "0              Paperwhite voyage, no regrets!  \n",
      "1           One Simply Could Not Ask For More  \n",
      "2  Great for those that just want an e-reader  \n",
      "3                    Love / Hate relationship  \n",
      "4                                   I LOVE IT  \n"
     ]
    }
   ],
   "source": [
    "# Simplify relevant columns names\n",
    "if('reviews.rating' in products.columns):\n",
    "    products['rating']=products['reviews.rating']\n",
    "    products.drop(['reviews.rating'],axis=1, inplace=True)\n",
    "\n",
    "if('reviews.text' in products.columns):\n",
    "    products['reviews']=products['reviews.text']\n",
    "    products.drop(['reviews.text'],axis=1, inplace=True)\n",
    "    \n",
    "if('reviews.title' in products.columns):\n",
    "    products['title']=products['reviews.title']\n",
    "    products.drop(['reviews.title'],axis=1, inplace=True)\n",
    "\n",
    "# Drop irrelevant columns\n",
    "relevant_cols=['reviews','rating','title']\n",
    "products = products.loc[:, relevant_cols]\n",
    "\n",
    "# Drop Nana\n",
    "products.dropna(subset=['rating', 'reviews','title'], inplace=True)\n",
    "products.reset_index(drop=True, inplace=True)\n",
    "\n",
    "print(products.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build the word count vector for each review"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us explore a specific example of a Amazon product."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I can read a lot longer without pain in my hands since the cover holds it for me. Very nice shade of blue.\n"
     ]
    }
   ],
   "source": [
    "print(products['reviews'][269])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we will perform 2 simple data transformations:\n",
    "\n",
    "1. Remove punctuation using [Python's built-in](https://docs.python.org/2/library/string.html) string functionality.\n",
    "2. Transform the reviews into word-counts.\n",
    "\n",
    "**Aside**. In this notebook, we remove all punctuations for the sake of simplicity. A smarter approach to punctuations would preserve phrases such as \"I'd\", \"would've\", \"hadn't\" and so forth. See [this page](https://neptune.ai/blog/tokenization-in-nlp) for an example of smart handling of punctuations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0       I initially had trouble deciding between the p...\n",
      "1       Allow me to preface this with a little history...\n",
      "2       I am enjoying it so far Great for reading Had ...\n",
      "3       I bought one of the first Paperwhites and have...\n",
      "4       I have to say upfront  I dont like coroporate ...\n",
      "                              ...                        \n",
      "1172    This is not the same remote that I got for my ...\n",
      "1173    I have had to change the batteries in this rem...\n",
      "1174    Remote did not activate nor did it connect to ...\n",
      "1175    It does the job but is super over priced I fee...\n",
      "1176    I ordered this item to replace the one that no...\n",
      "Name: reviews, Length: 1177, dtype: object\n"
     ]
    }
   ],
   "source": [
    "import string \n",
    "def remove_punctuation(text):\n",
    "    try: # python 2.x\n",
    "        text = text.translate(None, string.punctuation) \n",
    "    except: # python 3.x\n",
    "        translator = text.maketrans('', '', string.punctuation)\n",
    "        text = text.translate(translator)\n",
    "    return text\n",
    "\n",
    "products['reviews'] = products['reviews'].apply(remove_punctuation)\n",
    "print(products['reviews'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1177, 6750)\n",
      "(1177, 6750)\n",
      "                                             reviews  rating  \\\n",
      "0  I initially had trouble deciding between the p...     5.0   \n",
      "1  Allow me to preface this with a little history...     5.0   \n",
      "2  I am enjoying it so far Great for reading Had ...     4.0   \n",
      "3  I bought one of the first Paperwhites and have...     5.0   \n",
      "4  I have to say upfront  I dont like coroporate ...     5.0   \n",
      "\n",
      "                                        title  word_count_0  word_count_1  \\\n",
      "0              Paperwhite voyage, no regrets!           0.0           0.0   \n",
      "1           One Simply Could Not Ask For More           0.0           0.0   \n",
      "2  Great for those that just want an e-reader           0.0           0.0   \n",
      "3                    Love / Hate relationship           0.0           0.0   \n",
      "4                                   I LOVE IT           0.0           0.0   \n",
      "\n",
      "   word_count_2  word_count_3  word_count_4  word_count_5  word_count_6  ...  \\\n",
      "0           0.0           0.0           0.0           0.0           0.0  ...   \n",
      "1           0.0           0.0           0.0           0.0           0.0  ...   \n",
      "2           0.0           0.0           0.0           0.0           0.0  ...   \n",
      "3           0.0           0.0           0.0           0.0           0.0  ...   \n",
      "4           0.0           0.0           0.0           0.0           0.0  ...   \n",
      "\n",
      "   word_count_6740  word_count_6741  word_count_6742  word_count_6743  \\\n",
      "0              0.0              0.0              0.0              0.0   \n",
      "1              0.0              0.0              0.0              0.0   \n",
      "2              0.0              0.0              0.0              0.0   \n",
      "3              0.0              0.0              0.0              0.0   \n",
      "4              0.0              0.0              0.0              0.0   \n",
      "\n",
      "   word_count_6744  word_count_6745  word_count_6746  word_count_6747  \\\n",
      "0              0.0              0.0              0.0              0.0   \n",
      "1              0.0              0.0              0.0              0.0   \n",
      "2              0.0              0.0              0.0              0.0   \n",
      "3              0.0              0.0              0.0              0.0   \n",
      "4              0.0              0.0              0.0              0.0   \n",
      "\n",
      "   word_count_6748  word_count_6749  \n",
      "0              0.0              0.0  \n",
      "1              0.0              0.0  \n",
      "2              0.0              0.0  \n",
      "3              0.0              0.0  \n",
      "4              0.0              0.0  \n",
      "\n",
      "[5 rows x 6753 columns]\n"
     ]
    }
   ],
   "source": [
    "# Frequency counts\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "count_vect = CountVectorizer()\n",
    "X_train_counts = count_vect.fit_transform(products['reviews'])\n",
    "print(X_train_counts.shape)\n",
    "\n",
    "# TF-IDF\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "tfidf_transformer = TfidfTransformer()\n",
    "X_train_tfidf = tfidf_transformer.fit_transform(X_train_counts)\n",
    "print(X_train_tfidf.shape)\n",
    "\n",
    "word_count_df = pd.DataFrame(X_train_tfidf.toarray())\n",
    "word_count_df = word_count_df.add_prefix('word_count_')\n",
    "products = pd.concat([products, word_count_df], axis=1)\n",
    "\n",
    "print(products.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let us explore what the sample example above looks like after these 2 transformations. Here, each entry in the **word_count** column is a dictionary where the key is the word and the value is a count of the number of times the word occurs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract sentiments\n",
    "\n",
    "We will **ignore** all reviews with *rating = 3*, since they tend to have a neutral sentiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1053, 6753)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "products = products[products['rating'] != 3]\n",
    "products.reset_index(drop=True, inplace=True)\n",
    "products.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we will assign reviews with a rating of 4 or higher to be *positive* reviews, while the ones with rating of 2 or lower are *negative*. For the sentiment column, we use +1 for the positive class label and -1 for the negative class label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>reviews</th>\n",
       "      <th>rating</th>\n",
       "      <th>title</th>\n",
       "      <th>word_count_0</th>\n",
       "      <th>word_count_1</th>\n",
       "      <th>word_count_2</th>\n",
       "      <th>word_count_3</th>\n",
       "      <th>word_count_4</th>\n",
       "      <th>word_count_5</th>\n",
       "      <th>word_count_6</th>\n",
       "      <th>...</th>\n",
       "      <th>word_count_6741</th>\n",
       "      <th>word_count_6742</th>\n",
       "      <th>word_count_6743</th>\n",
       "      <th>word_count_6744</th>\n",
       "      <th>word_count_6745</th>\n",
       "      <th>word_count_6746</th>\n",
       "      <th>word_count_6747</th>\n",
       "      <th>word_count_6748</th>\n",
       "      <th>word_count_6749</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I initially had trouble deciding between the p...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>Paperwhite voyage, no regrets!</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Allow me to preface this with a little history...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>One Simply Could Not Ask For More</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I am enjoying it so far Great for reading Had ...</td>\n",
       "      <td>4.0</td>\n",
       "      <td>Great for those that just want an e-reader</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>I bought one of the first Paperwhites and have...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>Love / Hate relationship</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>I have to say upfront  I dont like coroporate ...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>I LOVE IT</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 6754 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             reviews  rating  \\\n",
       "0  I initially had trouble deciding between the p...     5.0   \n",
       "1  Allow me to preface this with a little history...     5.0   \n",
       "2  I am enjoying it so far Great for reading Had ...     4.0   \n",
       "3  I bought one of the first Paperwhites and have...     5.0   \n",
       "4  I have to say upfront  I dont like coroporate ...     5.0   \n",
       "\n",
       "                                        title  word_count_0  word_count_1  \\\n",
       "0              Paperwhite voyage, no regrets!           0.0           0.0   \n",
       "1           One Simply Could Not Ask For More           0.0           0.0   \n",
       "2  Great for those that just want an e-reader           0.0           0.0   \n",
       "3                    Love / Hate relationship           0.0           0.0   \n",
       "4                                   I LOVE IT           0.0           0.0   \n",
       "\n",
       "   word_count_2  word_count_3  word_count_4  word_count_5  word_count_6  ...  \\\n",
       "0           0.0           0.0           0.0           0.0           0.0  ...   \n",
       "1           0.0           0.0           0.0           0.0           0.0  ...   \n",
       "2           0.0           0.0           0.0           0.0           0.0  ...   \n",
       "3           0.0           0.0           0.0           0.0           0.0  ...   \n",
       "4           0.0           0.0           0.0           0.0           0.0  ...   \n",
       "\n",
       "   word_count_6741  word_count_6742  word_count_6743  word_count_6744  \\\n",
       "0              0.0              0.0              0.0              0.0   \n",
       "1              0.0              0.0              0.0              0.0   \n",
       "2              0.0              0.0              0.0              0.0   \n",
       "3              0.0              0.0              0.0              0.0   \n",
       "4              0.0              0.0              0.0              0.0   \n",
       "\n",
       "   word_count_6745  word_count_6746  word_count_6747  word_count_6748  \\\n",
       "0              0.0              0.0              0.0              0.0   \n",
       "1              0.0              0.0              0.0              0.0   \n",
       "2              0.0              0.0              0.0              0.0   \n",
       "3              0.0              0.0              0.0              0.0   \n",
       "4              0.0              0.0              0.0              0.0   \n",
       "\n",
       "   word_count_6749  sentiment  \n",
       "0              0.0          1  \n",
       "1              0.0          1  \n",
       "2              0.0          1  \n",
       "3              0.0          1  \n",
       "4              0.0          1  \n",
       "\n",
       "[5 rows x 6754 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "products['sentiment'] = products['rating'].apply(lambda r : +1 if r > 3 else -1)\n",
    "products.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[<AxesSubplot:title={'center':'sentiment'}>]], dtype=object)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX0AAAEICAYAAACzliQjAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8vihELAAAACXBIWXMAAAsTAAALEwEAmpwYAAAVy0lEQVR4nO3df7DddX3n8efLpEAlLQlgUwTGhJFiqTu1cqts7dREHA1017CzaHHqGtx0snata0s7S6y71XXXLXQ6ZXTq6GZFwV1LUFqXVLSKgTuOOw1bsQooqwT8lSyCIKDxB6K+94/zST2N9yb3nHPvSeDzfMzcud/v5/P5fr/v+zk3r/O933PON6kqJEl9eMLhLkCSND2GviR1xNCXpI4Y+pLUEUNfkjpi6EtSRwx9aYGSfCjJpsNdhzSJ+D596ccleQPw1Kp62RFQy5XAnqr6D4e7Fj32eaYvSR0x9PW4kOSSJHuTfDPJ55Kck+QJSbYmuSvJA0nem+T4Nn5NkkqyKcmXk9yf5HWtbwPwh8BvJNmX5NOtfTbJb7Xli5L87ySXJ3koyd1JfqW1fyXJfcOXgpIcneRP27HuTfL2JD/Z+tYl2ZPk99t29yR5RevbAvwm8O9bLX89zXnV44+hr8e8JGcAvwP8clX9FPBC4IvAq4HzgecCTwYeBN56wOa/CpwBnAP8UZKfr6q/Af4rcE1VraiqX5zn0M8GbgVOAP4C2A78MvBU4GXAnydZ0cZeCvwc8IzWfzLwR0P7+lnguNa+GXhrklVVtQ14D/AnrZZ/PtLkSAcw9PV48APgaODMJD9RVV+sqruAVwKvq6o9VfUI8AbggiTLh7b9T1X1nar6NPBpYL6An8sXqupdVfUD4BrgVOCNVfVIVX0E+B7w1CQBtgC/V1Vfr6pvMnhSuXBoX4+2bR+tqg8C+xg8GUmLavmhh0hHtqraneR3GYT6LyT5MHAx8BTg/Ul+ODT8B8DqofWvDi1/G1jBwt07tPydVsuBbSuAJwFPBG4Z5D8AAZYNjX2gqr4/QS3Sgnimr8eFqvqLqvpVBkFfwGXAV4Bzq2rl0NcxVbV3IbtcxPLuZ/AE8AtDdRxXVQsNdd9ip0Vj6OsxL8kZSZ6X5GjguwwC9ofA24E3JXlKG/ekJBsXuNt7gTVJJv43UlU/BP47cHmSn2m1nJzkhSPUctqkdUhg6Ovx4WgGL5Tez+Byzc8ArwXeDOwAPpLkm8AuBi++LsT72vcHknxyEWq8BNgN7EryDeCjLPya/RUMXq94KMn/WoRa1DE/nCVJHfFMX5I6YuhLUkcMfUnqiKEvSR05oj+cdeKJJ9aaNWvG3v5b3/oWxx577OIVtEisazTWNRrrGs3jsa5bbrnl/qp60pydVXXEfp111lk1iZtuummi7ZeKdY3GukZjXaN5PNYFfKLmyVUv70hSRw4Z+kne2W73evtQ2/FJbkhyZ/u+qrUnyVuS7E5ya5JnDm2zqY2/0/99SJIOj4Wc6V8JbDigbSuws6pOB3a2dYBzgdPb1xbgbTB4kgBez+DTkM8CXr//iUKSND2HDP2q+hjw9QOaNwJXteWrGNyzfH/7u9tlpV3AyiQnMbi/+Q01uK3sg8AN/PgTiSRpiS3oNgxJ1gAfqKqnt/WHqmplWw7wYFWtTPIB4NKq+njr28ngniPrgGOq6r+09v8IfKeq/nSOY21h8FcCq1evPmv79u1j/3D79u1jxYoj7+601jUa6xqNdY3m8VjX+vXrb6mqmbn6Jn7LZlVVkkW7gU8N/qegbQAzMzO1bt26sfc1OzvLJNsvFesajXWNxrpG01td475759522Yb2/b7WvpfB/x603ymtbb52SdIUjRv6O4D978DZBFw31P7y9i6es4GHq+oe4MPAC5Ksai/gvqC1SZKm6JCXd5JczeCa/IlJ9jB4F86lwHuTbAa+BLykDf8gcB6D+4Z/G3gFQFV9Pcl/Bv6ujXtjVR344rAkaYkdMvSr6qXzdJ0zx9gCXjXPft4JvHOk6iTpMFqz9frDduwrNyzNrSH8RK4kdcTQl6SOGPqS1BFDX5I6YuhLUkcMfUnqiKEvSR0x9CWpI4a+JHXE0Jekjhj6ktQRQ1+SOmLoS1JHDH1J6oihL0kdMfQlqSOGviR1xNCXpI4Y+pLUEUNfkjpi6EtSRwx9SeqIoS9JHTH0Jakjhr4kdcTQl6SOGPqS1BFDX5I6YuhLUkcMfUnqiKEvSR0x9CWpI4a+JHVkotBP8ntJPpPk9iRXJzkmydokNyfZneSaJEe1sUe39d2tf82i/ASSpAUbO/STnAz8O2Cmqp4OLAMuBC4DLq+qpwIPApvbJpuBB1v75W2cJGmKJr28sxz4ySTLgScC9wDPA65t/VcB57fljW2d1n9Okkx4fEnSCFJV42+cvAZ4E/Ad4CPAa4Bd7WyeJKcCH6qqpye5HdhQVXta313As6vq/gP2uQXYArB69eqztm/fPnZ9+/btY8WKFWNvv1SsazTWNRrrGs3B6rpt78NTruZH1h63bOz5Wr9+/S1VNTNX3/JxC0qyisHZ+1rgIeB9wIZx97dfVW0DtgHMzMzUunXrxt7X7Owsk2y/VKxrNNY1GusazcHqumjr9dMtZsiVG45dkvma5PLO84EvVNXXqupR4K+A5wAr2+UegFOAvW15L3AqQOs/DnhgguNLkkY0Seh/GTg7yRPbtflzgM8CNwEXtDGbgOva8o62Tuu/sSa5tiRJGtnYoV9VNzN4QfaTwG1tX9uAS4CLk+wGTgCuaJtcAZzQ2i8Gtk5QtyRpDGNf0weoqtcDrz+g+W7gWXOM/S7w4kmOJ0majJ/IlaSOGPqS1BFDX5I6YuhLUkcMfUnqiKEvSR0x9CWpI4a+JHXE0Jekjhj6ktQRQ1+SOmLoS1JHDH1J6oihL0kdMfQlqSOGviR1xNCXpI4Y+pLUEUNfkjpi6EtSRwx9SeqIoS9JHTH0Jakjhr4kdcTQl6SOGPqS1BFDX5I6YuhLUkcMfUnqiKEvSR0x9CWpI4a+JHXE0JekjkwU+klWJrk2yf9NckeSf5rk+CQ3JLmzfV/VxibJW5LsTnJrkmcuzo8gSVqoSc/03wz8TVU9DfhF4A5gK7Czqk4HdrZ1gHOB09vXFuBtEx5bkjSisUM/yXHArwFXAFTV96rqIWAjcFUbdhVwflveCLy7BnYBK5OcNO7xJUmjm+RMfy3wNeBdSf4+yTuSHAusrqp72pivAqvb8snAV4a239PaJElTkqoab8NkBtgFPKeqbk7yZuAbwKurauXQuAeralWSDwCXVtXHW/tO4JKq+sQB+93C4PIPq1evPmv79u1j1Qewb98+VqxYMfb2S8W6RmNdo7Gu0Rysrtv2Pjzlan5k7XHLxp6v9evX31JVM3P1LZ+gpj3Anqq6ua1fy+D6/b1JTqqqe9rlm/ta/17g1KHtT2lt/0hVbQO2AczMzNS6devGLnB2dpZJtl8q1jUa6xqNdY3mYHVdtPX66RYz5MoNxy7JfI19eaeqvgp8JckZrekc4LPADmBTa9sEXNeWdwAvb+/iORt4eOgykCRpCiY50wd4NfCeJEcBdwOvYPBE8t4km4EvAS9pYz8InAfsBr7dxkqSpmii0K+qTwFzXTc6Z46xBbxqkuNJkibjJ3IlqSOGviR1xNCXpI4Y+pLUEUNfkjpi6EtSRwx9SeqIoS9JHTH0Jakjhr4kdcTQl6SOGPqS1BFDX5I6YuhLUkcMfUnqiKEvSR0x9CWpI4a+JHXE0Jekjhj6ktQRQ1+SOmLoS1JHDH1J6oihL0kdMfQlqSOGviR1xNCXpI4Y+pLUEUNfkjpi6EtSRwx9SeqIoS9JHTH0Jakjhr4kdWTi0E+yLMnfJ/lAW1+b5OYku5Nck+So1n50W9/d+tdMemxJ0mgW40z/NcAdQ+uXAZdX1VOBB4HNrX0z8GBrv7yNkyRN0UShn+QU4NeBd7T1AM8Drm1DrgLOb8sb2zqt/5w2XpI0Jamq8TdOrgX+GPgp4A+Ai4Bd7WyeJKcCH6qqpye5HdhQVXta313As6vq/gP2uQXYArB69eqztm/fPnZ9+/btY8WKFWNvv1SsazTWNRrrGs3B6rpt78NTruZH1h63bOz5Wr9+/S1VNTNX3/JxC0ryz4D7quqWJOvG3c+BqmobsA1gZmam1q0bf9ezs7NMsv1Ssa7RWNdorGs0B6vroq3XT7eYIVduOHZJ5mvs0AeeA7woyXnAMcBPA28GViZZXlXfB04B9rbxe4FTgT1JlgPHAQ9McHxJ0ojGvqZfVa+tqlOqag1wIXBjVf0mcBNwQRu2CbiuLe9o67T+G2uSa0uSpJEtxfv0LwEuTrIbOAG4orVfAZzQ2i8Gti7BsSVJBzHJ5Z1/UFWzwGxbvht41hxjvgu8eDGOJ0kaj5/IlaSOGPqS1BFDX5I6YuhLUkcMfUnqiKEvSR0x9CWpI4a+JHXE0Jekjhj6ktQRQ1+SOmLoS1JHDH1J6oihL0kdMfQlqSOGviR1xNCXpI4Y+pLUEUNfkjpi6EtSRwx9SeqIoS9JHTH0Jakjhr4kdcTQl6SOGPqS1BFDX5I6YuhLUkcMfUnqiKEvSR0x9CWpI4a+JHXE0Jekjowd+klOTXJTks8m+UyS17T245PckOTO9n1Va0+StyTZneTWJM9crB9CkrQwk5zpfx/4/ao6EzgbeFWSM4GtwM6qOh3Y2dYBzgVOb19bgLdNcGxJ0hjGDv2quqeqPtmWvwncAZwMbASuasOuAs5vyxuBd9fALmBlkpPGPb4kaXSpqsl3kqwBPgY8HfhyVa1s7QEerKqVST4AXFpVH299O4FLquoTB+xrC4O/BFi9evVZ27dvH7uuffv2sWLFirG3XyrWNRrrGo11jeZgdd229+EpV/Mja49bNvZ8rV+//paqmpmrb/lEVQFJVgB/CfxuVX1jkPMDVVVJRnpWqaptwDaAmZmZWrdu3di1zc7OMsn2S8W6RmNdo7Gu0Rysrou2Xj/dYoZcueHYJZmvid69k+QnGAT+e6rqr1rzvfsv27Tv97X2vcCpQ5uf0tokSVMyybt3AlwB3FFVfzbUtQPY1JY3AdcNtb+8vYvnbODhqrpn3ONLkkY3yeWd5wD/Crgtyada2x8ClwLvTbIZ+BLwktb3QeA8YDfwbeAVExxbkjSGsUO/vSCbebrPmWN8Aa8a93iSpMn5iVxJ6oihL0kdMfQlqSOGviR1xNCXpI4Y+pLUEUNfkjpi6EtSRwx9SeqIoS9JHTH0Jakjhr4kdcTQl6SOGPqS1BFDX5I6YuhLUkcMfUnqiKEvSR0x9CWpI4a+JHXE0Jekjiw/3AUspdv2PsxFW6+f+nG/eOmvT/2YkrQQnulLUkcMfUnqiKEvSR0x9CWpI4a+JHXE0Jekjhj6ktQRQ1+SOmLoS1JHDH1J6oihL0kdMfQlqSNTD/0kG5J8LsnuJFunfXxJ6tlUQz/JMuCtwLnAmcBLk5w5zRokqWfTPtN/FrC7qu6uqu8B24GNU65Bkro17fvpnwx8ZWh9D/Ds4QFJtgBb2uq+JJ+b4HgnAvdPsP1YctkhhxyWuhbAukZjXaOxrhGsv2yiup4yX8cR95+oVNU2YNti7CvJJ6pqZjH2tZisazTWNRrrGk1vdU378s5e4NSh9VNamyRpCqYd+n8HnJ5kbZKjgAuBHVOuQZK6NdXLO1X1/SS/A3wYWAa8s6o+s4SHXJTLREvAukZjXaOxrtF0VVeqain2K0k6AvmJXEnqiKEvSR15TId+khcn+UySHyaZ961N8936ob2gfHNrv6a9uLwYdR2f5IYkd7bvq+YYsz7Jp4a+vpvk/NZ3ZZIvDPU9Y1p1tXE/GDr2jqH2wzlfz0jyt+3xvjXJbwz1Lep8HepWIUmObj//7jYfa4b6XtvaP5fkhZPUMUZdFyf5bJufnUmeMtQ352M6pbouSvK1oeP/1lDfpva435lk05Trunyops8neWiobynn651J7kty+zz9SfKWVvetSZ451Df5fFXVY/YL+HngDGAWmJlnzDLgLuA04Cjg08CZre+9wIVt+e3Aby9SXX8CbG3LW4HLDjH+eODrwBPb+pXABUswXwuqC9g3T/thmy/g54DT2/KTgXuAlYs9Xwf7fRka82+Bt7flC4Fr2vKZbfzRwNq2n2VTrGv90O/Qb++v62CP6ZTqugj48zm2PR64u31f1ZZXTauuA8a/msEbS5Z0vtq+fw14JnD7PP3nAR8CApwN3LyY8/WYPtOvqjuq6lCf2J3z1g9JAjwPuLaNuwo4f5FK29j2t9D9XgB8qKq+vUjHn8+odf2Dwz1fVfX5qrqzLf8/4D7gSYt0/GELuVXIcL3XAue0+dkIbK+qR6rqC8Dutr+p1FVVNw39Du1i8DmYpTbJrVVeCNxQVV+vqgeBG4ANh6mulwJXL9KxD6qqPsbgJG8+G4F318AuYGWSk1ik+XpMh/4CzXXrh5OBE4CHqur7B7QvhtVVdU9b/iqw+hDjL+THf+He1P60uzzJ0VOu65gkn0iya/8lJ46g+UryLAZnb3cNNS/WfM33+zLnmDYfDzOYn4Vsu5R1DdvM4Gxxv7ke02nW9S/b43Ntkv0f0Dwi5qtdBlsL3DjUvFTztRDz1b4o83XE3YbhQEk+CvzsHF2vq6rrpl3Pfgera3ilqirJvO+Lbc/g/4TBZxf2ey2D8DuKwXt1LwHeOMW6nlJVe5OcBtyY5DYGwTa2RZ6v/wFsqqoftuax5+vxKMnLgBnguUPNP/aYVtVdc+9h0f01cHVVPZLk3zD4K+l5Uzr2QlwIXFtVPxhqO5zztaSO+NCvqudPuIv5bv3wAIM/m5a3s7WRbglxsLqS3JvkpKq6p4XUfQfZ1UuA91fVo0P73n/W+0iSdwF/MM26qmpv+353klngl4C/5DDPV5KfBq5n8IS/a2jfY8/XHBZyq5D9Y/YkWQ4cx+D3aSlvM7KgfSd5PoMn0udW1SP72+d5TBcjxA5ZV1U9MLT6Dgav4ezfdt0B284uQk0LqmvIhcCrhhuWcL4WYr7aF2W+eri8M+etH2rwyshNDK6nA2wCFusvhx1tfwvZ749dS2zBt/86+vnAnK/yL0VdSVbtvzyS5ETgOcBnD/d8tcfu/QyudV57QN9iztdCbhUyXO8FwI1tfnYAF2bw7p61wOnA/5mglpHqSvJLwH8DXlRV9w21z/mYTrGuk4ZWXwTc0ZY/DLyg1bcKeAH/+C/eJa2r1fY0Bi+K/u1Q21LO10LsAF7e3sVzNvBwO7FZnPlaqleop/EF/AsG17UeAe4FPtzanwx8cGjcecDnGTxTv26o/TQG/yh3A+8Djl6kuk4AdgJ3Ah8Fjm/tM8A7hsatYfDs/YQDtr8RuI1BeP1PYMW06gJ+pR370+375iNhvoCXAY8Cnxr6esZSzNdcvy8MLhe9qC0f037+3W0+Thva9nVtu88B5y7y7/uh6vpo+3ewf352HOoxnVJdfwx8ph3/JuBpQ9v+6zaPu4FXTLOutv4G4NIDtlvq+bqawbvPHmWQX5uBVwKvbP1h8J9N3dWOPzO07cTz5W0YJKkjPVzekSQ1hr4kdcTQl6SOGPqS1BFDX5I6YuhLUkcMfUnqyP8HgQRm78Ye+S0AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# View number of positive and negative sentiment\n",
    "products.hist(column=['sentiment'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can see that the dataset contains an extra column called **sentiment** which is either positive (+1) or negative (-1).\n",
    "\n",
    "Note, there are significantly more positive reviews than negative reviews."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Match number of positive and negative reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 977 positive reviews\n",
      "There are 76 negative reviews\n",
      "[Update] There are 76 positive reviews\n",
      "                                               reviews  rating  \\\n",
      "43   After 15 months my 500 tablet is no longer use...     1.0   \n",
      "69   This is a great controller for our Fire TV I p...     2.0   \n",
      "79   This is a great controller for our Fire TV I p...     2.0   \n",
      "84   The connection drops out on this unit pretty b...     1.0   \n",
      "117  As the Kids Edition is nothing more than a bas...     1.0   \n",
      "\n",
      "                                                 title  word_count_0  \\\n",
      "43                                Dead after 15 months           0.0   \n",
      "69   Great controller with 2 major problems! Not re...           0.0   \n",
      "79   Great controller with 2 major problems! Not re...           0.0   \n",
      "84   This needs to be way better for the amount I s...           0.0   \n",
      "117          Freetime makes me angry. So, so angry. 3,           0.0   \n",
      "\n",
      "     word_count_1  word_count_2  word_count_3  word_count_4  word_count_5  \\\n",
      "43            0.0           0.0           0.0           0.0           0.0   \n",
      "69            0.0           0.0           0.0           0.0           0.0   \n",
      "79            0.0           0.0           0.0           0.0           0.0   \n",
      "84            0.0           0.0           0.0           0.0           0.0   \n",
      "117           0.0           0.0           0.0           0.0           0.0   \n",
      "\n",
      "     word_count_6  ...  word_count_6741  word_count_6742  word_count_6743  \\\n",
      "43            0.0  ...              0.0              0.0              0.0   \n",
      "69            0.0  ...              0.0              0.0              0.0   \n",
      "79            0.0  ...              0.0              0.0              0.0   \n",
      "84            0.0  ...              0.0              0.0              0.0   \n",
      "117           0.0  ...              0.0              0.0              0.0   \n",
      "\n",
      "     word_count_6744  word_count_6745  word_count_6746  word_count_6747  \\\n",
      "43               0.0              0.0              0.0              0.0   \n",
      "69               0.0              0.0              0.0              0.0   \n",
      "79               0.0              0.0              0.0              0.0   \n",
      "84               0.0              0.0              0.0              0.0   \n",
      "117              0.0              0.0              0.0              0.0   \n",
      "\n",
      "     word_count_6748  word_count_6749  sentiment  \n",
      "43               0.0              0.0         -1  \n",
      "69               0.0              0.0         -1  \n",
      "79               0.0              0.0         -1  \n",
      "84               0.0              0.0         -1  \n",
      "117              0.0              0.0         -1  \n",
      "\n",
      "[5 rows x 6754 columns]\n",
      "                                               reviews  rating  \\\n",
      "200  It seems to work fine so far 2 Days with it Wh...     4.0   \n",
      "816  I live in a small apartment and this fan kept ...     5.0   \n",
      "351  We got this the day it came out I have a four ...     5.0   \n",
      "292  Ill preface this by saying that I own an iPad ...     5.0   \n",
      "162  I was very hesitant to purchase a refurbished ...     5.0   \n",
      "\n",
      "                                                 title  word_count_0  \\\n",
      "200           I like it Amazon...But this is IMPORTANT           0.0   \n",
      "816                                 Like it very much.           0.0   \n",
      "351  This Fire Tablet is much Better than the Last ...           0.0   \n",
      "292  Unreal performance for 99! I couldn't be happi...           0.0   \n",
      "162       My refurb is like new. I love the Echo Dots!           0.0   \n",
      "\n",
      "     word_count_1  word_count_2  word_count_3  word_count_4  word_count_5  \\\n",
      "200           0.0           0.0           0.0           0.0           0.0   \n",
      "816           0.0           0.0           0.0           0.0           0.0   \n",
      "351           0.0           0.0           0.0           0.0           0.0   \n",
      "292           0.0           0.0           0.0           0.0           0.0   \n",
      "162           0.0           0.0           0.0           0.0           0.0   \n",
      "\n",
      "     word_count_6  ...  word_count_6741  word_count_6742  word_count_6743  \\\n",
      "200      0.000000  ...              0.0              0.0              0.0   \n",
      "816      0.000000  ...              0.0              0.0              0.0   \n",
      "351      0.000000  ...              0.0              0.0              0.0   \n",
      "292      0.000000  ...              0.0              0.0              0.0   \n",
      "162      0.128173  ...              0.0              0.0              0.0   \n",
      "\n",
      "     word_count_6744  word_count_6745  word_count_6746  word_count_6747  \\\n",
      "200              0.0              0.0              0.0              0.0   \n",
      "816              0.0              0.0              0.0              0.0   \n",
      "351              0.0              0.0              0.0              0.0   \n",
      "292              0.0              0.0              0.0              0.0   \n",
      "162              0.0              0.0              0.0              0.0   \n",
      "\n",
      "     word_count_6748  word_count_6749  sentiment  \n",
      "200              0.0              0.0          1  \n",
      "816              0.0              0.0          1  \n",
      "351              0.0              0.0          1  \n",
      "292              0.0              0.0          1  \n",
      "162              0.0              0.0          1  \n",
      "\n",
      "[5 rows x 6754 columns]\n"
     ]
    }
   ],
   "source": [
    "# Report number of positive examples\n",
    "positive_sent = products[products['sentiment']==1]\n",
    "print('There are {} positive reviews'.format(len(positive_sent)))\n",
    "\n",
    "# Report number of negative examples\n",
    "negative_sent = products[products['sentiment']==-1]\n",
    "print('There are {} negative reviews'.format(len(negative_sent)))\n",
    "\n",
    "# Sample number of negative example from positive examples (# positive > # negative)\n",
    "positive_sample = positive_sent.sample(n = len(negative_sent))\n",
    "print('[Update] There are {} positive reviews'.format(len(positive_sample)))\n",
    "\n",
    "# Merge positive and negative examples and update products dataframe\n",
    "frames = [negative_sent, positive_sample]\n",
    "products = pd.concat(frames)\n",
    "print(products.head())\n",
    "print(products.tail())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split data into training and test sets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's perform a train/test split with 80% of the data in the training set and 20% of the data in the test set. We use `seed=1` so that everyone gets the same result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "121\n",
      "121\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X, y = products.loc[:, ~products.columns.isin(['sentiment'])], products.loc[:, products.columns.isin(['sentiment'])]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.2, random_state=1)\n",
    "print(len(X_train))\n",
    "print(len(y_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train a sentiment classifier with logistic regression\n",
    "\n",
    "We will now use logistic regression to create a sentiment classifier on the training data. This model will use the column **word_count** as a feature and the column **sentiment** as the target. We will use `validation_set=None` to obtain same results as everyone else.\n",
    "\n",
    "**Note:** This line may take 1-2 minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word_count_0</th>\n",
       "      <th>word_count_1</th>\n",
       "      <th>word_count_2</th>\n",
       "      <th>word_count_3</th>\n",
       "      <th>word_count_4</th>\n",
       "      <th>word_count_5</th>\n",
       "      <th>word_count_6</th>\n",
       "      <th>word_count_7</th>\n",
       "      <th>word_count_8</th>\n",
       "      <th>word_count_9</th>\n",
       "      <th>...</th>\n",
       "      <th>word_count_6740</th>\n",
       "      <th>word_count_6741</th>\n",
       "      <th>word_count_6742</th>\n",
       "      <th>word_count_6743</th>\n",
       "      <th>word_count_6744</th>\n",
       "      <th>word_count_6745</th>\n",
       "      <th>word_count_6746</th>\n",
       "      <th>word_count_6747</th>\n",
       "      <th>word_count_6748</th>\n",
       "      <th>word_count_6749</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>229</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>543</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>404</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>457</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>897</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 6750 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     word_count_0  word_count_1  word_count_2  word_count_3  word_count_4  \\\n",
       "229           0.0           0.0           0.0           0.0           0.0   \n",
       "543           0.0           0.0           0.0           0.0           0.0   \n",
       "404           0.0           0.0           0.0           0.0           0.0   \n",
       "457           0.0           0.0           0.0           0.0           0.0   \n",
       "897           0.0           0.0           0.0           0.0           0.0   \n",
       "\n",
       "     word_count_5  word_count_6  word_count_7  word_count_8  word_count_9  \\\n",
       "229           0.0           0.0           0.0           0.0           0.0   \n",
       "543           0.0           0.0           0.0           0.0           0.0   \n",
       "404           0.0           0.0           0.0           0.0           0.0   \n",
       "457           0.0           0.0           0.0           0.0           0.0   \n",
       "897           0.0           0.0           0.0           0.0           0.0   \n",
       "\n",
       "     ...  word_count_6740  word_count_6741  word_count_6742  word_count_6743  \\\n",
       "229  ...              0.0              0.0              0.0              0.0   \n",
       "543  ...              0.0              0.0              0.0              0.0   \n",
       "404  ...              0.0              0.0              0.0              0.0   \n",
       "457  ...              0.0              0.0              0.0              0.0   \n",
       "897  ...              0.0              0.0              0.0              0.0   \n",
       "\n",
       "     word_count_6744  word_count_6745  word_count_6746  word_count_6747  \\\n",
       "229              0.0              0.0              0.0              0.0   \n",
       "543              0.0              0.0              0.0              0.0   \n",
       "404              0.0              0.0              0.0              0.0   \n",
       "457              0.0              0.0              0.0              0.0   \n",
       "897              0.0              0.0              0.0              0.0   \n",
       "\n",
       "     word_count_6748  word_count_6749  \n",
       "229              0.0              0.0  \n",
       "543              0.0              0.0  \n",
       "404              0.0              0.0  \n",
       "457              0.0              0.0  \n",
       "897              0.0              0.0  \n",
       "\n",
       "[5 rows x 6750 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_sentiment = X_train.loc[:,X_train.columns.str.startswith('word_count_')]\n",
    "X_test_sentiment = X_test.loc[:,X_test.columns.str.startswith('word_count_')]\n",
    "\n",
    "X_train_sentiment.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(random_state=0)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "sentiment_model = LogisticRegression(random_state=0)\n",
    "sentiment_model.fit(X_train_sentiment, np.ravel(y_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Aside**. You may get a warning to the effect of \"Terminated due to numerical difficulties --- this model may not be ideal\". It means that the quality metric (to be covered in Module 3) failed to improve in the last iteration of the run. The difficulty arises as the sentiment model puts too much weight on extremely rare words. A way to rectify this is to apply regularization, to be covered in Module 4. Regularization lessens the effect of extremely rare words. For the purpose of this assignment, however, please proceed with the model above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have fitted the model, we can extract the weights (coefficients) as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. ... 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "weights = sentiment_model.coef_\n",
    "print(weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are a total of `6750` coefficients in the model. Recall from the lecture that positive weights $w_j$ correspond to weights that cause positive sentiment, while negative weights correspond to negative sentiment. \n",
    "\n",
    "Fill in the following block of code to calculate how many *weights* are positive ( >= 0). (**Hint**: Use numpy to sum the weights and check *weights* must be positive ( >= 0))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of positive weights: 5675 \n",
      "Number of negative weights: 1075 \n"
     ]
    }
   ],
   "source": [
    "num_positive_weights = np.sum(weights >= 0)\n",
    "num_negative_weights = np.sum(weights < 0)\n",
    "\n",
    "print(\"Number of positive weights: %s \" % num_positive_weights)\n",
    "print(\"Number of negative weights: %s \" % num_negative_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Discussion Question:** How many weights are >= 0?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making predictions with logistic regression\n",
    "\n",
    "Now that a model is trained, we can make predictions on the **test data**. In this section, we will explore this in the context of 3 examples in the test dataset.  We refer to this set of 3 examples as the **sample_test_data**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "804    4.0\n",
      "384    4.0\n",
      "840    5.0\n",
      "Name: rating, dtype: float64\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>reviews</th>\n",
       "      <th>rating</th>\n",
       "      <th>title</th>\n",
       "      <th>word_count_0</th>\n",
       "      <th>word_count_1</th>\n",
       "      <th>word_count_2</th>\n",
       "      <th>word_count_3</th>\n",
       "      <th>word_count_4</th>\n",
       "      <th>word_count_5</th>\n",
       "      <th>word_count_6</th>\n",
       "      <th>...</th>\n",
       "      <th>word_count_6740</th>\n",
       "      <th>word_count_6741</th>\n",
       "      <th>word_count_6742</th>\n",
       "      <th>word_count_6743</th>\n",
       "      <th>word_count_6744</th>\n",
       "      <th>word_count_6745</th>\n",
       "      <th>word_count_6746</th>\n",
       "      <th>word_count_6747</th>\n",
       "      <th>word_count_6748</th>\n",
       "      <th>word_count_6749</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>804</th>\n",
       "      <td>If you already have the Amazon Echo system the...</td>\n",
       "      <td>4.0</td>\n",
       "      <td>Handy speaker if you don't need the best sound.</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>384</th>\n",
       "      <td>An Amazoncom official commented on this review...</td>\n",
       "      <td>4.0</td>\n",
       "      <td>Decent, inexpensive, entry-level tablet 5,942 ...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>840</th>\n",
       "      <td>We have bought two Taps one for ourselves and ...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>Great for music</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 6753 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               reviews  rating  \\\n",
       "804  If you already have the Amazon Echo system the...     4.0   \n",
       "384  An Amazoncom official commented on this review...     4.0   \n",
       "840  We have bought two Taps one for ourselves and ...     5.0   \n",
       "\n",
       "                                                 title  word_count_0  \\\n",
       "804    Handy speaker if you don't need the best sound.           0.0   \n",
       "384  Decent, inexpensive, entry-level tablet 5,942 ...           0.0   \n",
       "840                                    Great for music           0.0   \n",
       "\n",
       "     word_count_1  word_count_2  word_count_3  word_count_4  word_count_5  \\\n",
       "804           0.0           0.0           0.0           0.0           0.0   \n",
       "384           0.0           0.0           0.0           0.0           0.0   \n",
       "840           0.0           0.0           0.0           0.0           0.0   \n",
       "\n",
       "     word_count_6  ...  word_count_6740  word_count_6741  word_count_6742  \\\n",
       "804           0.0  ...              0.0              0.0              0.0   \n",
       "384           0.0  ...              0.0              0.0              0.0   \n",
       "840           0.0  ...              0.0              0.0              0.0   \n",
       "\n",
       "     word_count_6743  word_count_6744  word_count_6745  word_count_6746  \\\n",
       "804              0.0              0.0              0.0              0.0   \n",
       "384              0.0              0.0              0.0              0.0   \n",
       "840              0.0              0.0              0.0              0.0   \n",
       "\n",
       "     word_count_6747  word_count_6748  word_count_6749  \n",
       "804              0.0              0.0              0.0  \n",
       "384              0.0              0.0              0.0  \n",
       "840              0.0              0.0              0.0  \n",
       "\n",
       "[3 rows x 6753 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_test_data = X_test[10:13]\n",
    "print(sample_test_data['rating'])\n",
    "sample_test_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's dig deeper into the first row of the **sample_test_data**. Here's the full review:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'If you already have the Amazon Echo system then the Tap is a nice addition I use it primarily as an outdoor speaker for my patio when the weather is nice There are Bluetooth speakers on the market with better sound quality but it does an adequate job as a speaker The real advantage is being able to interface with the Echo system as well as item to music newsetc I have not used it off its charger long enough to comment on battery life but so far a six hour session outside was no problem'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_test_data['reviews'].iloc[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That review seems pretty positive.\n",
    "\n",
    "Now, let's see what the next row of the **sample_test_data** looks like. As we could guess from the sentiment (-1), the review is quite negative."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'An Amazoncom official commented on this reviewWhats this The Amazon team saysOctober 11 2013Thank you for taking the time to share your feedback Were excited to see your encouraging comments regarding display and sound and appreciate the thoughtful analysis you did of this device compared to the previous generation Kindle Fire and Kindle Fire HD As you correctly point out the new Kindle Fire HD is the successor to last years entry level Kindle Fire which does not include features such as camera microphone or HDMI port You are also correct to point out that this device does not support Mayday although there are still multiple ways to get help for free directly from your device We hope you enjoy the new features and services Thanks The Kindle Team'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_test_data['reviews'].iloc[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now make a **class** prediction for the **sample_test_data**. The `sentiment_model` should predict **+1** if the sentiment is positive and **-1** if the sentiment is negative. Recall from the lecture that the **score** (sometimes called **margin**) for the logistic regression model  is defined as:\n",
    "\n",
    "$$\n",
    "\\mbox{score}_i = \\mathbf{w}^T h(\\mathbf{x}_i)\n",
    "$$ \n",
    "\n",
    "where $h(\\mathbf{x}_i)$ represents the features for example $i$.  We will write some code to obtain the **scores**. For each row, the **score** (or margin) is a number in the range **[-inf, inf]**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.18001181  0.0690312  -0.00620248]\n"
     ]
    }
   ],
   "source": [
    "scores = sample_test_data.loc[:, sample_test_data.columns.str.startswith('word_count_')].to_numpy() @ weights.reshape((-1)) + sentiment_model.intercept_\n",
    "print(scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predicting sentiment\n",
    "\n",
    "These scores can be used to make class predictions as follows:\n",
    "\n",
    "$$\n",
    "\\hat{y} = \n",
    "\\left\\{\n",
    "\\begin{array}{ll}\n",
    "      +1 & \\mathbf{w}^T h(\\mathbf{x}_i) > 0 \\\\\n",
    "      -1 & \\mathbf{w}^T h(\\mathbf{x}_i) \\leq 0 \\\\\n",
    "\\end{array} \n",
    "\\right.\n",
    "$$\n",
    "\n",
    "Using scores, write code to calculate $\\hat{y}$, the class predictions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1  1 -1]\n"
     ]
    }
   ],
   "source": [
    "y_pred = np.where(scores > 0, 1, -1)\n",
    "print(y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the following code to verify that the class predictions obtained by your calculations are the same as that obtained above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class predictions:\n",
      "[ 1  1 -1]\n"
     ]
    }
   ],
   "source": [
    "print(\"Class predictions:\")\n",
    "print(sentiment_model.predict(\n",
    "    sample_test_data.loc[:, sample_test_data.columns.str.startswith('word_count_')]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Checkpoint**: Make sure your class predictions match with the one obtained from above.\n",
    "\n",
    "### Probability predictions\n",
    "\n",
    "Recall from the lectures that we can also calculate the probability predictions from the scores using:\n",
    "$$\n",
    "P(y_i = +1 | \\mathbf{x}_i,\\mathbf{w}) = \\frac{1}{1 + \\exp(-\\mathbf{w}^T h(\\mathbf{x}_i))}.\n",
    "$$\n",
    "\n",
    "Using the variable **scores** calculated previously, write code to calculate the probability that a sentiment is positive using the above formula. For each row, the probabilities should be a number in the range **[0, 1]**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.54488182 0.51725095 0.49844939]\n"
     ]
    }
   ],
   "source": [
    "y_prob = 1 / (1 + np.exp(-scores))\n",
    "print(y_prob)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Checkpoint**: Make sure your probability predictions match the ones obtained from sklearn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class predictions for y=1:\n",
      "[0.54488182 0.51725095 0.49844939]\n"
     ]
    }
   ],
   "source": [
    "print(\"Class predictions for y=1:\")\n",
    "print(sentiment_model.predict_proba(\n",
    "    sample_test_data.loc[:, sample_test_data.columns.str.startswith('word_count_')])[:, 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Discussion Question:** Of the three data points in **sample_test_data**, which one (first, second, or third) has the **lowest probability** of being classified as a positive review?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Find the most positive (and negative) review"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now turn to examining the full test dataset, **test_data**, and use Numpy argsort to form predictions on all of the test data points for faster performance.\n",
    "\n",
    "Using the `sentiment_model`, find the 20 reviews in the entire **test_data** with the **highest probability** of being classified as a **positive review**. We refer to these as the \"most positive reviews.\"\n",
    "\n",
    "To calculate these top-20 reviews, use the following steps:\n",
    "1.  Make probability predictions on **test_data** using the `sentiment_model`. (**Hint:** When you call `.predict` to make predictions on the test data.)\n",
    "2.  Sort the data according to those predictions and pick the top 20. (**Hint:** You can use indexing [-topn:] to find the top k rows sorted according to the value of a specified column.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[11 22 17 16  4 12 15 27 23 26 25  7  8  3 30  9 14  2 28  0]\n",
      "                                                reviews  rating  \\\n",
      "384   An Amazoncom official commented on this review...     4.0   \n",
      "96    I am writing this from the perspective of bein...     4.0   \n",
      "500   Same features as Echo now but easier to move a...     5.0   \n",
      "217                                                 Too     2.0   \n",
      "469   Just really thought it would have been more us...     2.0   \n",
      "840   We have bought two Taps one for ourselves and ...     5.0   \n",
      "158   Love the new Kindle Priced right with an extra...     5.0   \n",
      "842   I was not overly impressed with this system I ...     2.0   \n",
      "39    I am updating my review from two months ago My...     4.0   \n",
      "267   In this day and age of rectangles with screens...     2.0   \n",
      "776   hard to use and does not support Guam online i...     1.0   \n",
      "213                                     very overpriced     2.0   \n",
      "968   As good as the Echo is the Echo Voice Remote s...     2.0   \n",
      "206   Amazon billed my credit card prematurely on pa...     4.0   \n",
      "591   Im thinking this must of been a fluke it never...     2.0   \n",
      "876   Could not get it to work properly I put the ap...     1.0   \n",
      "134   DONT BUY a reburbished Kindle Ive had to retur...     1.0   \n",
      "1046  Dont bother paying for one or getting a replac...     2.0   \n",
      "971   Worked great for a few weeks then completely s...     1.0   \n",
      "1033  I dont know whats wrong with fire tv remotes b...     1.0   \n",
      "\n",
      "                                                  title  word_count_0  \\\n",
      "384   Decent, inexpensive, entry-level tablet 5,942 ...           0.0   \n",
      "96                  Good Value at 79 with a Few Nits 5,           0.0   \n",
      "500                   Awesome device w/ Alexa features.           0.0   \n",
      "217                                           Two Stars           0.0   \n",
      "469                                       Not impressed           0.0   \n",
      "840                                     Great for music           0.0   \n",
      "158                              Love the new Kindle 8!           0.0   \n",
      "842                             I returned this product           0.0   \n",
      "39                   Updated review (after 2 months)...           0.0   \n",
      "267                                            Just Bad           0.0   \n",
      "776                                  Not a good product           0.0   \n",
      "213                                           Two Stars           0.0   \n",
      "968   Has Trouble Understanding Commands, Volume Con...           0.0   \n",
      "206                  amazon billing deceptive practices           0.0   \n",
      "591                                     It never worked           0.0   \n",
      "876                            Would not work properly!           0.0   \n",
      "134                     Don't buy a refurbished Kindle!           0.0   \n",
      "1046                                      Don't Buy It.           0.0   \n",
      "971                                  Dead after 3 weeks           0.0   \n",
      "1033                       Amazon fire tv remote sucks!           0.0   \n",
      "\n",
      "      word_count_1  word_count_2  word_count_3  word_count_4  word_count_5  \\\n",
      "384            0.0           0.0           0.0           0.0           0.0   \n",
      "96             0.0           0.0           0.0           0.0           0.0   \n",
      "500            0.0           0.0           0.0           0.0           0.0   \n",
      "217            0.0           0.0           0.0           0.0           0.0   \n",
      "469            0.0           0.0           0.0           0.0           0.0   \n",
      "840            0.0           0.0           0.0           0.0           0.0   \n",
      "158            0.0           0.0           0.0           0.0           0.0   \n",
      "842            0.0           0.0           0.0           0.0           0.0   \n",
      "39             0.0           0.0           0.0           0.0           0.0   \n",
      "267            0.0           0.0           0.0           0.0           0.0   \n",
      "776            0.0           0.0           0.0           0.0           0.0   \n",
      "213            0.0           0.0           0.0           0.0           0.0   \n",
      "968            0.0           0.0           0.0           0.0           0.0   \n",
      "206            0.0           0.0           0.0           0.0           0.0   \n",
      "591            0.0           0.0           0.0           0.0           0.0   \n",
      "876            0.0           0.0           0.0           0.0           0.0   \n",
      "134            0.0           0.0           0.0           0.0           0.0   \n",
      "1046           0.0           0.0           0.0           0.0           0.0   \n",
      "971            0.0           0.0           0.0           0.0           0.0   \n",
      "1033           0.0           0.0           0.0           0.0           0.0   \n",
      "\n",
      "      word_count_6  ...  word_count_6740  word_count_6741  word_count_6742  \\\n",
      "384            0.0  ...              0.0              0.0              0.0   \n",
      "96             0.0  ...              0.0              0.0              0.0   \n",
      "500            0.0  ...              0.0              0.0              0.0   \n",
      "217            0.0  ...              0.0              0.0              0.0   \n",
      "469            0.0  ...              0.0              0.0              0.0   \n",
      "840            0.0  ...              0.0              0.0              0.0   \n",
      "158            0.0  ...              0.0              0.0              0.0   \n",
      "842            0.0  ...              0.0              0.0              0.0   \n",
      "39             0.0  ...              0.0              0.0              0.0   \n",
      "267            0.0  ...              0.0              0.0              0.0   \n",
      "776            0.0  ...              0.0              0.0              0.0   \n",
      "213            0.0  ...              0.0              0.0              0.0   \n",
      "968            0.0  ...              0.0              0.0              0.0   \n",
      "206            0.0  ...              0.0              0.0              0.0   \n",
      "591            0.0  ...              0.0              0.0              0.0   \n",
      "876            0.0  ...              0.0              0.0              0.0   \n",
      "134            0.0  ...              0.0              0.0              0.0   \n",
      "1046           0.0  ...              0.0              0.0              0.0   \n",
      "971            0.0  ...              0.0              0.0              0.0   \n",
      "1033           0.0  ...              0.0              0.0              0.0   \n",
      "\n",
      "      word_count_6743  word_count_6744  word_count_6745  word_count_6746  \\\n",
      "384               0.0              0.0              0.0              0.0   \n",
      "96                0.0              0.0              0.0              0.0   \n",
      "500               0.0              0.0              0.0              0.0   \n",
      "217               0.0              0.0              0.0              0.0   \n",
      "469               0.0              0.0              0.0              0.0   \n",
      "840               0.0              0.0              0.0              0.0   \n",
      "158               0.0              0.0              0.0              0.0   \n",
      "842               0.0              0.0              0.0              0.0   \n",
      "39                0.0              0.0              0.0              0.0   \n",
      "267               0.0              0.0              0.0              0.0   \n",
      "776               0.0              0.0              0.0              0.0   \n",
      "213               0.0              0.0              0.0              0.0   \n",
      "968               0.0              0.0              0.0              0.0   \n",
      "206               0.0              0.0              0.0              0.0   \n",
      "591               0.0              0.0              0.0              0.0   \n",
      "876               0.0              0.0              0.0              0.0   \n",
      "134               0.0              0.0              0.0              0.0   \n",
      "1046              0.0              0.0              0.0              0.0   \n",
      "971               0.0              0.0              0.0              0.0   \n",
      "1033              0.0              0.0              0.0              0.0   \n",
      "\n",
      "      word_count_6747  word_count_6748  word_count_6749  \n",
      "384               0.0              0.0              0.0  \n",
      "96                0.0              0.0              0.0  \n",
      "500               0.0              0.0              0.0  \n",
      "217               0.0              0.0              0.0  \n",
      "469               0.0              0.0              0.0  \n",
      "840               0.0              0.0              0.0  \n",
      "158               0.0              0.0              0.0  \n",
      "842               0.0              0.0              0.0  \n",
      "39                0.0              0.0              0.0  \n",
      "267               0.0              0.0              0.0  \n",
      "776               0.0              0.0              0.0  \n",
      "213               0.0              0.0              0.0  \n",
      "968               0.0              0.0              0.0  \n",
      "206               0.0              0.0              0.0  \n",
      "591               0.0              0.0              0.0  \n",
      "876               0.0              0.0              0.0  \n",
      "134               0.0              0.0              0.0  \n",
      "1046              0.0              0.0              0.0  \n",
      "971               0.0              0.0              0.0  \n",
      "1033              0.0              0.0              0.0  \n",
      "\n",
      "[20 rows x 6753 columns]\n"
     ]
    }
   ],
   "source": [
    "# Calculate the class probabilities for the test set\n",
    "y_prob = sentiment_model.predict_proba(X_test_sentiment)\n",
    "\n",
    "# Sort the test set in descending order of their probabilities of being positive\n",
    "idx = np.argsort(-y_prob[:, 1])\n",
    "\n",
    "# Get the indices of the 20 most positive reviews\n",
    "idx_most_positive = idx[-20:] \n",
    "print(idx_most_positive)\n",
    "\n",
    "# Get the corresponding reviews from the test set\n",
    "most_positive_reviews = X_test.iloc[idx_most_positive]\n",
    "print(most_positive_reviews)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Discussion Question**: Which of the following products are represented in the 20 most positive reviews? [multiple choice]\n",
    "\n",
    "\n",
    "Now, let us repeat this exercise to find the \"most negative reviews.\" Use the prediction probabilities to find the  20 reviews in the **test_data** with the **lowest probability** of being classified as a **positive review**. Repeat the same steps above but make sure you **sort in the opposite order**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                               reviews  rating  \\\n",
      "746  This is a great addition to my expanding smart...     5.0   \n",
      "422  To sum up what I will tell you about in the de...     4.0   \n",
      "597  Hi Everyone Alexa is the best of the best I fe...     5.0   \n",
      "465  I have the Amazon dot which is great But after...     5.0   \n",
      "433  This is the middle model of the three models t...     5.0   \n",
      "582  Overall sound is not as good as Echo my primar...     4.0   \n",
      "571  It is a great addition to my home office My Al...     5.0   \n",
      "512  Love the Dot Hate the Tap Portable This device...     2.0   \n",
      "403  So I dont normally write reviews but I just ha...     5.0   \n",
      "804  If you already have the Amazon Echo system the...     4.0   \n",
      "382  An Amazoncom official commented on this review...     4.0   \n",
      "384  An Amazoncom official commented on this review...     4.0   \n",
      "96   I am writing this from the perspective of bein...     4.0   \n",
      "500  Same features as Echo now but easier to move a...     5.0   \n",
      "217                                                Too     2.0   \n",
      "469  Just really thought it would have been more us...     2.0   \n",
      "840  We have bought two Taps one for ourselves and ...     5.0   \n",
      "158  Love the new Kindle Priced right with an extra...     5.0   \n",
      "842  I was not overly impressed with this system I ...     2.0   \n",
      "39   I am updating my review from two months ago My...     4.0   \n",
      "\n",
      "                                                 title  word_count_0  \\\n",
      "746                               Awesome smart device           0.0   \n",
      "422                  Nice Tablet with Sharp Display 4,           0.0   \n",
      "597                                       I Love Alexa           0.0   \n",
      "465                         This product is amazing!!!           0.0   \n",
      "433  Excellent 3rd-generation tablet that compares ...           0.0   \n",
      "582                             Good companion to Echo           0.0   \n",
      "571                                                Tap           0.0   \n",
      "512                Love the Dot, Hate the Tap Portable           0.0   \n",
      "403                        Best 50 I ever spent!!!! 1,           0.0   \n",
      "804    Handy speaker if you don't need the best sound.           0.0   \n",
      "382  Decent, inexpensive, entry-level tablet 5,946 ...           0.0   \n",
      "384  Decent, inexpensive, entry-level tablet 5,942 ...           0.0   \n",
      "96                 Good Value at 79 with a Few Nits 5,           0.0   \n",
      "500                  Awesome device w/ Alexa features.           0.0   \n",
      "217                                          Two Stars           0.0   \n",
      "469                                      Not impressed           0.0   \n",
      "840                                    Great for music           0.0   \n",
      "158                             Love the new Kindle 8!           0.0   \n",
      "842                            I returned this product           0.0   \n",
      "39                  Updated review (after 2 months)...           0.0   \n",
      "\n",
      "     word_count_1  word_count_2  word_count_3  word_count_4  word_count_5  \\\n",
      "746           0.0           0.0           0.0           0.0           0.0   \n",
      "422           0.0           0.0           0.0           0.0           0.0   \n",
      "597           0.0           0.0           0.0           0.0           0.0   \n",
      "465           0.0           0.0           0.0           0.0           0.0   \n",
      "433           0.0           0.0           0.0           0.0           0.0   \n",
      "582           0.0           0.0           0.0           0.0           0.0   \n",
      "571           0.0           0.0           0.0           0.0           0.0   \n",
      "512           0.0           0.0           0.0           0.0           0.0   \n",
      "403           0.0           0.0           0.0           0.0           0.0   \n",
      "804           0.0           0.0           0.0           0.0           0.0   \n",
      "382           0.0           0.0           0.0           0.0           0.0   \n",
      "384           0.0           0.0           0.0           0.0           0.0   \n",
      "96            0.0           0.0           0.0           0.0           0.0   \n",
      "500           0.0           0.0           0.0           0.0           0.0   \n",
      "217           0.0           0.0           0.0           0.0           0.0   \n",
      "469           0.0           0.0           0.0           0.0           0.0   \n",
      "840           0.0           0.0           0.0           0.0           0.0   \n",
      "158           0.0           0.0           0.0           0.0           0.0   \n",
      "842           0.0           0.0           0.0           0.0           0.0   \n",
      "39            0.0           0.0           0.0           0.0           0.0   \n",
      "\n",
      "     word_count_6  ...  word_count_6740  word_count_6741  word_count_6742  \\\n",
      "746           0.0  ...              0.0         0.000000              0.0   \n",
      "422           0.0  ...              0.0         0.000000              0.0   \n",
      "597           0.0  ...              0.0         0.000000              0.0   \n",
      "465           0.0  ...              0.0         0.000000              0.0   \n",
      "433           0.0  ...              0.0         0.104449              0.0   \n",
      "582           0.0  ...              0.0         0.000000              0.0   \n",
      "571           0.0  ...              0.0         0.000000              0.0   \n",
      "512           0.0  ...              0.0         0.000000              0.0   \n",
      "403           0.0  ...              0.0         0.062299              0.0   \n",
      "804           0.0  ...              0.0         0.000000              0.0   \n",
      "382           0.0  ...              0.0         0.000000              0.0   \n",
      "384           0.0  ...              0.0         0.000000              0.0   \n",
      "96            0.0  ...              0.0         0.000000              0.0   \n",
      "500           0.0  ...              0.0         0.000000              0.0   \n",
      "217           0.0  ...              0.0         0.000000              0.0   \n",
      "469           0.0  ...              0.0         0.000000              0.0   \n",
      "840           0.0  ...              0.0         0.000000              0.0   \n",
      "158           0.0  ...              0.0         0.000000              0.0   \n",
      "842           0.0  ...              0.0         0.000000              0.0   \n",
      "39            0.0  ...              0.0         0.000000              0.0   \n",
      "\n",
      "     word_count_6743  word_count_6744  word_count_6745  word_count_6746  \\\n",
      "746              0.0              0.0         0.000000              0.0   \n",
      "422              0.0              0.0         0.000000              0.0   \n",
      "597              0.0              0.0         0.000000              0.0   \n",
      "465              0.0              0.0         0.000000              0.0   \n",
      "433              0.0              0.0         0.000000              0.0   \n",
      "582              0.0              0.0         0.000000              0.0   \n",
      "571              0.0              0.0         0.000000              0.0   \n",
      "512              0.0              0.0         0.000000              0.0   \n",
      "403              0.0              0.0         0.087209              0.0   \n",
      "804              0.0              0.0         0.000000              0.0   \n",
      "382              0.0              0.0         0.000000              0.0   \n",
      "384              0.0              0.0         0.000000              0.0   \n",
      "96               0.0              0.0         0.000000              0.0   \n",
      "500              0.0              0.0         0.000000              0.0   \n",
      "217              0.0              0.0         0.000000              0.0   \n",
      "469              0.0              0.0         0.000000              0.0   \n",
      "840              0.0              0.0         0.000000              0.0   \n",
      "158              0.0              0.0         0.000000              0.0   \n",
      "842              0.0              0.0         0.000000              0.0   \n",
      "39               0.0              0.0         0.000000              0.0   \n",
      "\n",
      "     word_count_6747  word_count_6748  word_count_6749  \n",
      "746              0.0              0.0              0.0  \n",
      "422              0.0              0.0              0.0  \n",
      "597              0.0              0.0              0.0  \n",
      "465              0.0              0.0              0.0  \n",
      "433              0.0              0.0              0.0  \n",
      "582              0.0              0.0              0.0  \n",
      "571              0.0              0.0              0.0  \n",
      "512              0.0              0.0              0.0  \n",
      "403              0.0              0.0              0.0  \n",
      "804              0.0              0.0              0.0  \n",
      "382              0.0              0.0              0.0  \n",
      "384              0.0              0.0              0.0  \n",
      "96               0.0              0.0              0.0  \n",
      "500              0.0              0.0              0.0  \n",
      "217              0.0              0.0              0.0  \n",
      "469              0.0              0.0              0.0  \n",
      "840              0.0              0.0              0.0  \n",
      "158              0.0              0.0              0.0  \n",
      "842              0.0              0.0              0.0  \n",
      "39               0.0              0.0              0.0  \n",
      "\n",
      "[20 rows x 6753 columns]\n"
     ]
    }
   ],
   "source": [
    "# Get the indices of the 20 most negative reviews\n",
    "idx_most_negative = idx[:20]\n",
    "\n",
    "# Get the corresponding reviews from the test set\n",
    "most_negative_reviews = X_test.iloc[idx_most_negative]\n",
    "print(most_negative_reviews)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "**Question**: Which of the following products are represented in the 20 most negative reviews?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute accuracy of the classifier\n",
    "\n",
    "We will now evaluate the accuracy of the trained classifier. Recall that the accuracy is given by\n",
    "\n",
    "\n",
    "$$\n",
    "\\mbox{accuracy} = \\frac{\\mbox{# correctly classified examples}}{\\mbox{# total examples}}\n",
    "$$\n",
    "\n",
    "This can be computed as follows:\n",
    "\n",
    "* **Step 1:** Use the trained model to compute class predictions (**Hint:** Use the `predict` method)\n",
    "* **Step 2:** Count the number of data points when the predicted class labels match the ground truth labels (called `true_labels` below).\n",
    "* **Step 3:** Divide the total number of correct predictions by the total number of data points in the dataset.\n",
    "\n",
    "Complete the function below to compute the classification accuracy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def get_classification_accuracy(model, data, true_labels):\n",
    "    # First get the predictions\n",
    "    ## YOUR CODE HERE\n",
    "    pred = model.predict(data)\n",
    "    \n",
    "    # Compute the number of correctly classified examples\n",
    "    ## YOUR CODE HERE\n",
    "    num_correct = np.sum(pred == true_labels.to_numpy().reshape(-1))\n",
    "\n",
    "    # Then compute accuracy by dividing num_correct by total number of examples\n",
    "    ## YOUR CODE HERE\n",
    "    accuracy = num_correct / len(data)\n",
    "    \n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's compute the classification accuracy of the **sentiment_model** on the **test_data**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7741935483870968"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_classification_accuracy(sentiment_model, X_test_sentiment, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Discussion Question**: What is the accuracy of the **sentiment_model** on the **test_data**? Round your answer to 2 decimal places (e.g. 0.76).\n",
    "\n",
    "**Discussion Question**: Does a higher accuracy value on the **training_data** always imply that the classifier is better?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learn another classifier with fewer words\n",
    "\n",
    "There were a lot of words in the model we trained above. We will now train a simpler logistic regression model using only a subset of words that occur in the reviews. For this assignment, we selected a 20 words to work with. These are:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "significant_words = ['love', 'great', 'easy', 'old', 'amazing', 'perfect', 'loves', \n",
    "      'well', 'able', 'car', 'broke', 'less', 'even', 'waste', 'disappointed', \n",
    "      'work', 'product', 'money', 'would', 'return']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(significant_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#all_words = count_vect.get_feature_names_out() # newer version of sklearn\n",
    "all_words = count_vect.get_feature_names()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each review, we will use the **word_count** column and trim out all words that are **not** in the **significant_words** list above. We will use the [SArray dictionary trim by keys functionality]( https://dato.com/products/create/docs/generated/graphlab.SArray.dict_trim_by_keys.html). Note that we are performing this on both the training and test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word_count_310</th>\n",
       "      <th>word_count_515</th>\n",
       "      <th>word_count_1016</th>\n",
       "      <th>word_count_1137</th>\n",
       "      <th>word_count_1838</th>\n",
       "      <th>word_count_2039</th>\n",
       "      <th>word_count_2192</th>\n",
       "      <th>word_count_2764</th>\n",
       "      <th>word_count_3522</th>\n",
       "      <th>word_count_3662</th>\n",
       "      <th>word_count_3666</th>\n",
       "      <th>word_count_3901</th>\n",
       "      <th>word_count_4164</th>\n",
       "      <th>word_count_4416</th>\n",
       "      <th>word_count_4680</th>\n",
       "      <th>word_count_5069</th>\n",
       "      <th>word_count_6489</th>\n",
       "      <th>word_count_6538</th>\n",
       "      <th>word_count_6648</th>\n",
       "      <th>word_count_6671</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>229</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.108244</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.155978</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>543</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.149315</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>404</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.045696</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.041285</td>\n",
       "      <td>0.032184</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.056221</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>457</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>897</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     word_count_310  word_count_515  word_count_1016  word_count_1137  \\\n",
       "229             0.0             0.0              0.0              0.0   \n",
       "543             0.0             0.0              0.0              0.0   \n",
       "404             0.0             0.0              0.0              0.0   \n",
       "457             0.0             0.0              0.0              0.0   \n",
       "897             0.0             0.0              0.0              0.0   \n",
       "\n",
       "     word_count_1838  word_count_2039  word_count_2192  word_count_2764  \\\n",
       "229              0.0              0.0              0.0         0.000000   \n",
       "543              0.0              0.0              0.0         0.149315   \n",
       "404              0.0              0.0              0.0         0.000000   \n",
       "457              0.0              0.0              0.0         0.000000   \n",
       "897              0.0              0.0              0.0         0.000000   \n",
       "\n",
       "     word_count_3522  word_count_3662  word_count_3666  word_count_3901  \\\n",
       "229         0.000000              0.0              0.0         0.108244   \n",
       "543         0.000000              0.0              0.0         0.000000   \n",
       "404         0.045696              0.0              0.0         0.000000   \n",
       "457         0.000000              0.0              0.0         0.000000   \n",
       "897         0.000000              0.0              0.0         0.000000   \n",
       "\n",
       "     word_count_4164  word_count_4416  word_count_4680  word_count_5069  \\\n",
       "229              0.0         0.000000         0.000000              0.0   \n",
       "543              0.0         0.000000         0.000000              0.0   \n",
       "404              0.0         0.041285         0.032184              0.0   \n",
       "457              0.0         0.000000         0.000000              0.0   \n",
       "897              0.0         0.000000         0.000000              0.0   \n",
       "\n",
       "     word_count_6489  word_count_6538  word_count_6648  word_count_6671  \n",
       "229         0.155978              0.0              0.0         0.000000  \n",
       "543         0.000000              0.0              0.0         0.000000  \n",
       "404         0.000000              0.0              0.0         0.056221  \n",
       "457         0.000000              0.0              0.0         0.000000  \n",
       "897         0.000000              0.0              0.0         0.000000  "
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Extract significant words and split train/test sets\n",
    "\n",
    "sig_word_set = set(significant_words)\n",
    "sig_idx = [i for i, e in enumerate(all_words) if e in sig_word_set]\n",
    "\n",
    "X_train_sig = X_train_sentiment.iloc[:, sig_idx]\n",
    "X_test_sig = X_test_sentiment.iloc[:, sig_idx]\n",
    "\n",
    "X_train_sig.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see what the first example of the dataset looks like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I typically dont post reviews however dont waste your money on this case I got it on Friday and started using it that evening Its now Thursday and the case is peeling at the edges the fabric around the camera hole has started bubbling and the felt on the inside edge is wearing off i have not had any issues with it standing on its own like other people have mentioned I am giving this a two star mainly because it is still functional I am tempted to send it back but I like the functionality of this case'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train['reviews'].iloc[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **word_count** column had been working with before looks like the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "word_count_0       0.0\n",
       "word_count_1       0.0\n",
       "word_count_2       0.0\n",
       "word_count_3       0.0\n",
       "word_count_4       0.0\n",
       "                  ... \n",
       "word_count_6745    0.0\n",
       "word_count_6746    0.0\n",
       "word_count_6747    0.0\n",
       "word_count_6748    0.0\n",
       "word_count_6749    0.0\n",
       "Name: 229, Length: 6750, dtype: float64"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_sentiment.iloc[0, :]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we are only working with a subset of these words, the column **word_count_subset** is a subset of the above dictionary. In this example, only 2 `significant words` are present in this review."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "word_count_310     0.000000\n",
       "word_count_515     0.000000\n",
       "word_count_1016    0.000000\n",
       "word_count_1137    0.000000\n",
       "word_count_1838    0.000000\n",
       "word_count_2039    0.000000\n",
       "word_count_2192    0.000000\n",
       "word_count_2764    0.000000\n",
       "word_count_3522    0.000000\n",
       "word_count_3662    0.000000\n",
       "word_count_3666    0.000000\n",
       "word_count_3901    0.108244\n",
       "word_count_4164    0.000000\n",
       "word_count_4416    0.000000\n",
       "word_count_4680    0.000000\n",
       "word_count_5069    0.000000\n",
       "word_count_6489    0.155978\n",
       "word_count_6538    0.000000\n",
       "word_count_6648    0.000000\n",
       "word_count_6671    0.000000\n",
       "Name: 229, dtype: float64"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_sig.iloc[0, :]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train a logistic regression model on a subset of data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now build a classifier with **word_count_subset** as the feature and **sentiment** as the target. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression(random_state=0)\n"
     ]
    }
   ],
   "source": [
    "simple_model = LogisticRegression(random_state=0)\n",
    "simple_model.fit(X_train_sig, np.ravel(y_train))\n",
    "\n",
    "print(simple_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can compute the classification accuracy using the `get_classification_accuracy` function you implemented earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5806451612903226"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_classification_accuracy(simple_model, X_test_sig, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we will inspect the weights (coefficients) of the **simple_model**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.10478966  0.13355858  0.          0.03031667 -0.19878492  0.45147947\n",
      "  -0.07119773  1.02081719 -0.04144197  0.61359567  0.29898731 -0.22158574\n",
      "   0.16055619  0.0930607   0.15536263 -0.22411594 -0.425665    0.00498569\n",
      "  -0.28874078 -0.14998954]]\n"
     ]
    }
   ],
   "source": [
    "print(simple_model.coef_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's sort the coefficients (in descending order) by the **value** to obtain the coefficients with the most positive effect on the sentiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.02081719  0.61359567  0.45147947  0.29898731  0.16055619  0.15536263\n",
      "   0.13355858  0.0930607   0.03031667  0.00498569  0.         -0.04144197\n",
      "  -0.07119773 -0.10478966 -0.14998954 -0.19878492 -0.22158574 -0.22411594\n",
      "  -0.28874078 -0.425665  ]]\n"
     ]
    }
   ],
   "source": [
    "print(-np.sort(-simple_model.coef_, ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Discussion Question**: Consider the coefficients of **simple_model**. There should be 21 of them, an intercept term + one for each word in **significant_words**. How many of the 20 coefficients (corresponding to the 20 **significant_words** and *excluding the intercept term*) are positive for the `simple_model`?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11\n"
     ]
    }
   ],
   "source": [
    "num_positive_weights = np.sum(simple_model.coef_ >= 0)\n",
    "print(num_positive_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Discussion Question**: Are the positive words in the **simple_model** (let us call them `positive_significant_words`) also positive words in the **sentiment_model**?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparing models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now compare the accuracy of the **sentiment_model** and the **simple_model** using the `get_classification_accuracy` method you implemented above.\n",
    "\n",
    "First, compute the classification accuracy of the **sentiment_model** on the **train_data**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9917355371900827\n"
     ]
    }
   ],
   "source": [
    "sentiment_model_train_acc = get_classification_accuracy(sentiment_model, X_train_sentiment, y_train)\n",
    "print(sentiment_model_train_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, compute the classification accuracy of the **simple_model** on the **train_data**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6694214876033058\n"
     ]
    }
   ],
   "source": [
    "simple_model_test_acc = get_classification_accuracy(simple_model, X_train_sig, y_train)\n",
    "print(simple_model_test_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Discussion Question**: Which model (**sentiment_model** or **simple_model**) has higher accuracy on the TRAINING set?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we will repeat this exercise on the **test_data**. Start by computing the classification accuracy of the **sentiment_model** on the **test_data**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7741935483870968\n"
     ]
    }
   ],
   "source": [
    "sentiment_model_test_acc = get_classification_accuracy(sentiment_model, X_test_sentiment, y_test)\n",
    "print(sentiment_model_test_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will compute the classification accuracy of the **simple_model** on the **test_data**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5806451612903226"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "simple_model_test_acc = get_classification_accuracy(simple_model, X_test_sig, y_test)\n",
    "simple_model_test_acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Discussion Question**: Which model (**sentiment_model** or **simple_model**) has higher accuracy on the TEST set?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Discussion Question**: Comment out the sention on 'Match number of positive and negative reviews' and re-run the notebook. Which model (**sentiment_model** or **simple_model**) has higher accuracy on the TEST set? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline: Majority class prediction\n",
    "\n",
    "It is quite common to use the **majority class classifier** as the a baseline (or reference) model for comparison with your classifier model. The majority classifier model predicts the majority class for all data points. At the very least, you should healthily beat the majority class classifier, otherwise, the model is (usually) pointless.\n",
    "\n",
    "What is the majority class in the **train_data**?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "59\n",
      "62\n"
     ]
    }
   ],
   "source": [
    "num_positive = int(np.sum(y_train == +1))\n",
    "num_negative = int(np.sum(y_train == -1))\n",
    "print(num_positive)\n",
    "print(num_negative)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now compute the accuracy of the majority class classifier on **test_data**.\n",
    "\n",
    "**Discussion Question**: Enter the accuracy of the majority class classifier model on the **test_data**. Round your answer to two decimal places (e.g. 0.76)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sentiment    0.451613\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "if num_positive >= num_negative:\n",
    "    y_pred = 1\n",
    "else:\n",
    "    y_pred = -1\n",
    "\n",
    "n_correct = np.sum(y_test == y_pred)\n",
    "accuracy = n_correct / len(X_test)\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Discussion Question**: Is the **sentiment_model** definitely better than the majority class classifier (the baseline)?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Logistic Regression via Stochastic Gradient Descent\n",
    "\n",
    "The goal of this notebook is to implement a logistic regression classifier using stochastic gradient descent. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define hyperparameters for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_iter=1000           # number of passes on training data\n",
    "tol=1e-3                # stopping criteria for iterations\n",
    "penalty='l2'            # 'l1' and 'l2' regularization term\n",
    "alpha=0.001             # Constant that multiplies the regularization term. Ranges from [0 Inf)\n",
    "loss='log'              # 'log' is logistic regression, 'hinge' for Support Vector Machine\n",
    "random_state=42         # seed of the pseudo random number generated which is used while shuffling the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create SGD model using Sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SGDClassifier(alpha=0.001, loss='log', random_state=42)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "# Create SGD model\n",
    "sgd_model = SGDClassifier(max_iter=max_iter, \n",
    "                  tol=tol,\n",
    "                  penalty=penalty,\n",
    "                  alpha=alpha,\n",
    "                  loss=loss,\n",
    "                  random_state=random_state)\n",
    "sgd_model.fit(X_train_sentiment, np.ravel(y_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evalaute the SGD model on the sentiment test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-1,  1, -1, -1,  1,  1,  1, -1, -1, -1,  1,  1,  1,  1, -1,  1,  1,\n",
       "        1,  1,  1,  1,  1,  1, -1,  1, -1, -1,  1, -1,  1, -1])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentiment_predictions = sgd_model.predict(X_test_sentiment.loc[:, X_test_sentiment.columns.str.startswith('word_count_')])\n",
    "sentiment_predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inspect SGD coefficients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 0., 0., 0.]])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sgd_model.coef_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stochastic Gradient Descent with Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stochastic Gradient Descent with Cross Validation\n",
    "from sklearn.model_selection import RepeatedKFold\n",
    "from sklearn.model_selection import cross_val_score, cross_validate\n",
    "\n",
    "# evaluation method\n",
    "n_splits=4\n",
    "n_repeats=5\n",
    "sgd_cv = RepeatedKFold(n_splits=n_splits, n_repeats=n_repeats, random_state=1)\n",
    "\n",
    "# define model\n",
    "sgd_model = SGDClassifier(max_iter=max_iter, \n",
    "                              tol=tol,\n",
    "                              penalty=penalty,\n",
    "                              alpha=alpha,\n",
    "                              loss=loss,\n",
    "                              random_state=random_state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate cross validation scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. ... 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "sgd_results = cross_validate(sgd_model, X_train_sentiment, np.ravel(y_train), scoring='accuracy',cv=sgd_cv, n_jobs=-1, return_estimator=True)\n",
    "\n",
    "best_model_idx = np.argmin(sgd_results['test_score'])\n",
    "estimator = sgd_results['estimator'][best_model_idx]\n",
    "print(estimator.coef_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.742 (0.069)\n"
     ]
    }
   ],
   "source": [
    "# force scores to be positive\n",
    "scores = abs(sgd_results['test_score'])\n",
    "print('Accuracy: %.3f (%.3f)' % (np.mean(scores), np.std(scores)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Discussion Question** Explore effects of L2 regularization\n",
    "\n",
    "Now that we have written up all the pieces needed for regularized logistic regression, let's explore the benefits of using **L2 regularization** in analyzing sentiment for product reviews. **As iterations pass, the log likelihood should increase**.\n",
    "\n",
    "Below, we train models with increasing amounts of regularization, starting with no L2 penalty, which is equivalent to our previous logistic regression implementation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Confusion Matrix\n",
    "\n",
    "The accuracy, while convenient, does not tell the whole story. For a fuller picture, we turn to the **confusion matrix**. In the case of binary classification, the confusion matrix is a 2-by-2 matrix laying out correct and incorrect predictions made in each label as follows:\n",
    "```\n",
    "              +---------------------------------------------+\n",
    "              |                Predicted label              |\n",
    "              +----------------------+----------------------+\n",
    "              |          (-1)        |         (+1)         |\n",
    "+-------+-----+----------------------+----------------------+\n",
    "| True  |(-1) | # of true negative  | # of false positive |\n",
    "| label +-----+----------------------+----------------------+\n",
    "|       |(+1) | # of false negative | # of true positive  |\n",
    "+-------+-----+----------------------+----------------------+\n",
    "```\n",
    "To print out the confusion matrix for a classifier, use `metric='confusion_matrix'`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[11,  3],\n",
       "       [ 4, 13]])"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "sentiment_predictions = sentiment_model.predict(X_test_sentiment.loc[:, X_test_sentiment.columns.str.startswith('word_count_')])\n",
    "cmatrix = confusion_matrix(y_test, sentiment_predictions)\n",
    "cmatrix\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Discussion Question**: How many predicted values in the **test set** are **false positives**?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 3 false positives\n",
      "There are 4 false negatives\n",
      "There are 13 true positives\n",
      "There are 11 true negatives\n"
     ]
    }
   ],
   "source": [
    "# [Ans]\n",
    "true_neg, false_pos, false_neg, true_pos = cmatrix.ravel()\n",
    "print('There are {} false positives'.format(false_pos))\n",
    "print('There are {} false negatives'.format(false_neg))\n",
    "print('There are {} true positives'.format(true_pos))\n",
    "print('There are {} true negatives'.format(true_neg))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Computing the cost of mistakes\n",
    "\n",
    "\n",
    "Put yourself in the shoes of a manufacturer that sells a product on Amazon.com and you want to monitor your product's reviews in order to respond to complaints.  Even a few negative reviews may generate a lot of bad publicity about the product. So you don't want to miss any reviews with negative sentiments --- you'd rather put up with false alarms about potentially negative reviews instead of missing negative reviews entirely. In other words, **false positives cost more than false negatives**. (It may be the other way around for other scenarios, but let's stick with the manufacturer's scenario for now.)\n",
    "\n",
    "Suppose you know the costs involved in each kind of mistake: \n",
    "1. \\$100 for each false positive.\n",
    "2. \\$1 for each false negative.\n",
    "3. Correctly classified reviews incur no cost.\n",
    "\n",
    "**Discussion Question**: Given the stipulation, what is the cost associated with the logistic regression classifier's performance on the **test set**?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False positives costs $300, false negatives cost $4 with a total cost of $304\n"
     ]
    }
   ],
   "source": [
    "fp_cost = false_pos*100\n",
    "fn_cost = false_neg*1\n",
    "print('False positives costs ${}, false negatives cost ${} with a total cost of ${}'.format(fp_cost,fn_cost,fp_cost+fn_cost))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Precision and Recall\n",
    "\n",
    "You may not have exact dollar amounts for each kind of mistake. Instead, you may simply prefer to reduce the percentage of false positives to be less than, say, 3.5% of all positive predictions. This is where **precision** comes in:\n",
    "\n",
    "$$\n",
    "[\\text{precision}] = \\frac{[\\text{# positive data points with positive predicitions}]}{\\text{[# all data points with positive predictions]}} = \\frac{[\\text{# true positives}]}{[\\text{# true positives}] + [\\text{# false positives}]}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So to keep the percentage of false positives below 3.5% of positive predictions, we must raise the precision to 96.5% or higher. \n",
    "\n",
    "**First**, let us compute the precision of the logistic regression classifier on the **test_data**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision on test data: 0.8125\n"
     ]
    }
   ],
   "source": [
    "precision = true_pos/(true_pos+false_pos)\n",
    "print(\"Precision on test data: %s\" % precision)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Discussion Question**: Out of all reviews in the **test set** that are predicted to be positive, what fraction of them are **false positives**? (Round to the second decimal place e.g. 0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.1875"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "false_pos / (true_pos + false_pos)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Discussion Question:** Based on what we learned in lecture, if we wanted to reduce this fraction of false positives to be below 3.5%, we would (select one):\n",
    "\n",
    "- Discard a sufficient number of positive predictions\n",
    "- Discard a sufficient number of negative predictins\n",
    "- Increase threshold for predicting the positive class ($y_{hat} = +1$)\n",
    "- Decrease threshold for predicting the positive class ($y_{hat} = +1$)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A complementary metric is **recall**, which measures the ratio between the number of true positives and that of (ground-truth) positive reviews:\n",
    "\n",
    "$$\n",
    "[\\text{recall}] = \\frac{[\\text{# positive data points with positive predicitions}]}{\\text{[# all positive data points]}} = \\frac{[\\text{# true positives}]}{[\\text{# true positives}] + [\\text{# false negatives}]}\n",
    "$$\n",
    "\n",
    "Let us compute the recall on the **test_data**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recall on test data: 0.7647058823529411\n"
     ]
    }
   ],
   "source": [
    "recall = true_pos / (true_pos + false_neg)\n",
    "print(\"Recall on test data: %s\" % recall)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Discussion Question**: What fraction of the positive reviews in the **test_set** were correctly predicted as positive by the classifier?\n",
    "\n",
    "**Discussion Question**: What is the recall value for a classifier that predicts **+1** for all data points in the **test_data**?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7647058823529411"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "true_pos / (true_pos + false_neg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Precision-recall tradeoff\n",
    "\n",
    "In this part, we will explore the trade-off between precision and recall discussed in the lecture.  We first examine what happens when we use a different threshold value for making class predictions.  We then explore a range of threshold values and plot the associated precision-recall curve.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Varying the threshold\n",
    "\n",
    "False positives are costly in our example, so we may want to be more conservative about making positive predictions. To achieve this, instead of thresholding class probabilities at 0.5, we can choose a higher threshold. \n",
    "\n",
    "Write a function called `apply_threshold` that accepts two things\n",
    "* `probabilities` (an SArray of probability values)\n",
    "* `threshold` (a float between 0 and 1).\n",
    "\n",
    "The function should return an SArray, where each element is set to +1 or -1 depending whether the corresponding probability exceeds `threshold`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_threshold(probabilities, threshold):\n",
    "    ### YOUR CODE GOES HERE\n",
    "    # +1 if >= threshold and -1 otherwise.\n",
    "    return np.array([1 if p[1] >= threshold else -1 for p in probabilities])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run prediction with `output_type='probability'` to get the list of probability values. Then use thresholds set at 0.5 (default) and 0.9 to make predictions from these probability values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "probabilities = sentiment_model.predict_proba(X_test_sentiment)\n",
    "predictions_with_default_threshold = apply_threshold(probabilities, 0.5)\n",
    "predictions_with_high_threshold = apply_threshold(probabilities, 0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of positive predicted reviews (threshold = 0.5): 16\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of positive predicted reviews (threshold = 0.5): %s\" % (predictions_with_default_threshold == 1).sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of positive predicted reviews (threshold = 0.9): 0\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of positive predicted reviews (threshold = 0.9): %s\" % (predictions_with_high_threshold == 1).sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Discussion Question**: What happens to the number of positive predicted reviews as the threshold increased from 0.5 to 0.9?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploring the associated precision and recall as the threshold varies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By changing the probability threshold, it is possible to influence precision and recall. We can explore this as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import recall_score, precision_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Threshold = 0.5\n",
    "precision_with_default_threshold = precision_score(\n",
    "    y_test, predictions_with_default_threshold)\n",
    "\n",
    "recall_with_default_threshold = recall_score(\n",
    "    y_test, predictions_with_default_threshold\n",
    ")\n",
    "\n",
    "\n",
    "# Threshold = 0.9\n",
    "precision_with_high_threshold = precision_score(\n",
    "    y_test, predictions_with_high_threshold, zero_division=1)\n",
    "\n",
    "recall_with_high_threshold = recall_score(\n",
    "    y_test, predictions_with_high_threshold\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision (threshold = 0.5): 0.8125\n",
      "Recall (threshold = 0.5)   : 0.7647058823529411\n"
     ]
    }
   ],
   "source": [
    "print(\"Precision (threshold = 0.5): %s\" % precision_with_default_threshold)\n",
    "print(\"Recall (threshold = 0.5)   : %s\" % recall_with_default_threshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision (threshold = 0.9): 1.0\n",
      "Recall (threshold = 0.9)   : 0.0\n"
     ]
    }
   ],
   "source": [
    "print(\"Precision (threshold = 0.9): %s\" % precision_with_high_threshold)\n",
    "print(\"Recall (threshold = 0.9)   : %s\" % recall_with_high_threshold)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Discussion Question (variant 1)**: Does the **precision** increase with a higher threshold?\n",
    "\n",
    "**Discussion Question (variant 2)**: Does the **recall** increase with a higher threshold?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Precision-recall curve\n",
    "\n",
    "Now, we will explore various different values of tresholds, compute the precision and recall scores, and then plot the precision-recall curve."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold_values = np.linspace(0.5, 1, num=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each of the values of threshold, we compute the precision and recall scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "precision_all = []\n",
    "recall_all = []\n",
    "\n",
    "probabilities = sentiment_model.predict_proba(X_test_sentiment)\n",
    "for threshold in threshold_values:\n",
    "    predictions = apply_threshold(probabilities, threshold)\n",
    "\n",
    "    precision = precision_score(y_test, predictions, zero_division=1)\n",
    "\n",
    "    recall = recall_score(y_test, predictions)\n",
    "    \n",
    "    precision_all.append(precision)\n",
    "    recall_all.append(recall)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's plot the precision-recall curve to visualize the precision-recall tradeoff as we vary the threshold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 504x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "def plot_pr_curve(precision, recall, title):\n",
    "    plt.rcParams['figure.figsize'] = 7, 5\n",
    "    plt.locator_params(axis = 'x', nbins = 5)\n",
    "    plt.plot(recall, precision, 'b-', linewidth=4.0, color = '#B0017F')\n",
    "    plt.title(title)\n",
    "    plt.ylabel('Precision')\n",
    "    plt.xlabel('Recall')\n",
    "    plt.rcParams.update({'font.size': 16})\n",
    "    \n",
    "plot_pr_curve(precision_all, recall_all, 'Precision recall curve (all)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Discussion Question**: Among all the threshold values tried, what is the **smallest** threshold value that achieves a precision of 96.5% or better? Round your answer to 3 decimal places."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5555555555555556\n"
     ]
    }
   ],
   "source": [
    "for idx, precision in enumerate(precision_all):\n",
    "    if precision >= 0.965:\n",
    "        print(threshold_values[idx])\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Discussion Question**: Using `threshold` = 0.98, how many **false negatives** do we get on the **test_data**? (**Hint**: You may use the Sklearn precision and recall functions.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions = apply_threshold(probabilities, 0.98)\n",
    "cm = confusion_matrix(y_test, predictions)\n",
    "cm[1][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the number of false negatives (i.e the number of reviews to look at when not needed) that we have to deal with using this classifier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluating specific search terms\n",
    "\n",
    "So far, we looked at the number of false positives for the **entire test set**. In this section, let's select reviews using a specific search term and optimize the precision on these reviews only. After all, a manufacturer would be interested in tuning the false positive rate just for their products (the reviews they want to read) rather than that of the entire set of products on Amazon.\n",
    "\n",
    "## Precision-Recall on all tv related items\n",
    "\n",
    "From the **test set**, select all the reviews for all products with the word 'tv' in them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "tv_reviews = X_test_sentiment[X_test['reviews'].apply(lambda x: 'tv' in x.lower())]\n",
    "tv_y = y_test[X_test['reviews'].apply(lambda x: 'tv' in x.lower())]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's predict the probability of classifying these reviews as positive:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "probabilities = sentiment_model.predict_proba(tv_reviews)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plot the precision-recall curve for the **tv_reviews** dataset.\n",
    "\n",
    "**First**, let's consider the following `threshold_values` ranging from 0.5 to 1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold_values = np.linspace(0.5, 1, num=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Second**, as we did above, let's compute precision and recall for each value in `threshold_values` on the **tv_reviews** dataset.  Complete the code block below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "precision_all = []\n",
    "recall_all = []\n",
    "\n",
    "for threshold in threshold_values:    \n",
    "    # Make predictions. Use the `apply_threshold` function \n",
    "    ## YOUR CODE HERE \n",
    "    predictions = apply_threshold(probabilities, threshold)\n",
    "\n",
    "    # Calculate the precision.\n",
    "    # YOUR CODE HERE\n",
    "    precision = precision_score(tv_y, predictions, zero_division=1)\n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    recall = recall_score(tv_y, predictions)\n",
    "    \n",
    "    # Append the precision and recall scores.\n",
    "    precision_all.append(precision)\n",
    "    recall_all.append(recall)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Discussion Question**: Among all the threshold values tried, what is the **smallest** threshold value that achieves a precision of 96.5% or better for the reviews of data in **products**? Round your answer to 3 decimal places."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5\n"
     ]
    }
   ],
   "source": [
    "for idx, precision in enumerate(precision_all):\n",
    "    if precision >= 0.965:\n",
    "        print(threshold_values[idx])\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Discussion Question:** Is this threshold value smaller or larger than the threshold used for the entire dataset to achieve the same specified precision of 96.5%?\n",
    "\n",
    "**Finally**, let's plot the precision recall curve."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 504x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_pr_curve(precision_all, recall_all, \"Precision-Recall (tv)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What do you notice about the precision vs. recall values?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "vscode": {
   "interpreter": {
    "hash": "85463d1bb2888979eba71184c8d462df281b59dd3aecce6d9ffe8745b11b3329"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
